{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13324279,"sourceType":"datasetVersion","datasetId":8447139}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:13:23.340974Z","iopub.execute_input":"2025-10-17T17:13:23.341641Z","iopub.status.idle":"2025-10-17T17:13:23.350665Z","shell.execute_reply.started":"2025-10-17T17:13:23.341614Z","shell.execute_reply":"2025-10-17T17:13:23.349939Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/emotion/emotion-emotion_69k.csv\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# **TASK 1 - Preprocessing**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport json\nfrom collections import Counter\nimport os\nimport unicodedata  # For unicode normalization in text cleaning\n\n# ---------- Paths ----------\nif os.path.exists('/kaggle/input'):\n    data_path = '/kaggle/input/emotion/emotion-emotion_69k.csv'\n    output_dir = '/kaggle/working'\nelse:\n    data_path = 'dataset/emotion-emotion_69k.csv'\n    output_dir = '.'\n\n# ---------- Configuration ----------\nMAX_SEQ_LENGTH = 128  # Cap combined sequences (recommended for project)\nMIN_WORD_FREQ = 2     # Minimum frequency to include word in vocab (reduces vocab size, improves stability)\n\nSPECIAL_TOKENS = {\n    '<pad>': 0,\n    '<bos>': 1,\n    '<eos>': 2,\n    '<unk>': 3\n}\n\nprint(\"=\"*70)\nprint(\"TASK 1: PREPROCESSING FOR EMPATHETIC DIALOGUES\")\nprint(\"=\"*70)\nprint(f\"✓ Required modules loaded: pandas, re, json, unicodedata\")\n\n\n# ---------- Load dataset ----------\nprint(\"\\n📂 Loading dataset...\")\ndf = pd.read_csv(data_path)\nprint(f\"✓ Data loaded: {df.shape[0]} samples\")\nprint(f\"✓ Columns: {df.columns.tolist()}\")\n\n# Validate dataset structure\nneeded_columns = {\"Situation\", \"emotion\", \"empathetic_dialogues\", \"labels\"}\nmissing = [c for c in needed_columns if c not in df.columns]\nif missing:\n    print(f\"⚠️  Expected columns missing: {missing}\")\n    print(\"   Make sure you're using the Empathetic Dialogues dataset (not a classification CSV).\")\n    raise ValueError(f\"Missing required columns: {missing}\")\n\n# Filter out rows with missing critical data\ndf = df.dropna(subset=['Situation', 'emotion', 'empathetic_dialogues', 'labels'])\ndf = df.reset_index(drop=True)\nprint(f\"✓ After removing NaN: {df.shape[0]} samples\")\n\n# ---------- Parse empathetic dialogues ----------\nprint(\"\\n🔍 Parsing empathetic dialogues...\")\n\ndef extract_customer_utterance(dialogue_text):\n    \"\"\"\n    Extract the last customer utterance from the dialogue.\n    Format: \"Customer :utterance1\\nAgent :response1\\nCustomer :utterance2\\nAgent :\"\n    We want the last customer utterance before the final \"Agent :\"\n    Robust to spacing/casing variations.\n    \"\"\"\n    if pd.isna(dialogue_text):\n        return \"\"\n    \n    # Split by newlines\n    lines = dialogue_text.strip().split('\\n')\n    \n    # Find the last \"Customer :\" line (case-insensitive, flexible spacing)\n    customer_utterances = []\n    for line in lines:\n        # Check for customer prefix (case-insensitive)\n        line_lower = line.strip().lower()\n        if line_lower.startswith('customer :') or line_lower.startswith('customer:'):\n            # Extract everything after \"Customer :\" or \"Customer:\"\n            if ':' in line:\n                utterance = line.split(':', 1)[1].strip()\n                customer_utterances.append(utterance)\n    \n    # Return the last customer utterance (the one we need to respond to)\n    if customer_utterances:\n        return customer_utterances[-1]\n    return \"\"\n\n# Extract customer utterances\ndf['customer_utterance'] = df['empathetic_dialogues'].apply(extract_customer_utterance)\n\n# Check if extraction worked\nprint(f\"✓ Extracted customer utterances\")\nprint(f\"✓ Sample: '{df['customer_utterance'].iloc[0][:80]}...'\")\n\n# Filter out rows where we couldn't extract customer utterance\ndf = df[df['customer_utterance'].str.len() > 0]\ndf = df.reset_index(drop=True)\nprint(f\"✓ After filtering empty utterances: {df.shape[0]} samples\")\n\n# ---------- Hard Text Normalization ----------\ndef normalize_unicode(text):\n    \"\"\"Normalize unicode to NFKC form (cleans weird widths & punctuation)\"\"\"\n    return unicodedata.normalize(\"NFKC\", text)\n\ndef remove_emojis(text):\n    \"\"\"Remove emojis and other non-standard unicode characters\"\"\"\n    return ''.join(char for char in text if unicodedata.category(char)[0] != 'C' \n                   and unicodedata.category(char) not in ['So', 'Sk'])\n\ndef standardize_quotes(text):\n    \"\"\"Standardize different quote types to simple double quotes\"\"\"\n    # Replace smart quotes, curly quotes, etc.\n    quote_chars = ['\\u201c', '\\u201d', '\\u2018', '\\u2019', '`', '\\u00b4', '\\u2032']\n    for char in quote_chars:\n        text = text.replace(char, '\"')\n    return text\n\ndef clean_text_hard(text):\n    \"\"\"Hard normalization: lowercase, collapse spaces, standardize quotes, strip emojis\"\"\"\n    if pd.isna(text):\n        return \"\"\n    \n    # Convert to string\n    text = str(text)\n    \n    # Unicode normalization (NFKC cleans weird widths & punctuation)\n    text = normalize_unicode(text)\n    \n    # Lowercase\n    text = text.lower()\n    \n    # Remove emojis\n    text = remove_emojis(text)\n    \n    # Standardize quotes\n    text = standardize_quotes(text)\n    \n    # Remove extra punctuation repetitions (e.g., \"!!!\" -> \"!\")\n    text = re.sub(r'([.!?]){2,}', r'\\1', text)\n    text = re.sub(r'([,;:]){2,}', r'\\1', text)\n    \n    # Add space around punctuation for better tokenization\n    text = re.sub(r'([.,!?;:])', r' \\1 ', text)\n    \n    # Collapse multiple spaces into one\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Strip leading/trailing whitespace\n    text = text.strip()\n    \n    return text\n\ndef truncate_sequence(text, max_len=MAX_SEQ_LENGTH):\n    \"\"\"Truncate text to max_len tokens\"\"\"\n    tokens = text.split()\n    if len(tokens) > max_len:\n        tokens = tokens[:max_len]\n    return ' '.join(tokens)\n\n# ---------- Apply normalization ----------\nprint(\"\\n🧹 Applying hard normalization...\")\n# Note: We normalize but DON'T truncate yet - truncation happens after templating\ndf['Situation'] = df['Situation'].apply(clean_text_hard)\ndf['customer_utterance'] = df['customer_utterance'].apply(clean_text_hard)\ndf['labels'] = df['labels'].apply(clean_text_hard)\ndf['emotion'] = df['emotion'].str.lower().str.strip()\n\n# Filter out invalid emotions (but allow multi-word emotions with spaces)\ndf = df[df['emotion'].str.len() < 50]\ndf = df[df['emotion'].str.len() > 0]\ndf = df.reset_index(drop=True)\n\nprint(f\"✓ After filtering emotions: {df.shape[0]} samples\")\nprint(f\"✓ Unique emotions: {df['emotion'].nunique()}\")\n\n# ---------- Split dataset (80/10/10) ----------\nprint(\"\\n📊 Splitting dataset...\")\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\nn = len(df)\ntrain = df[:int(0.8*n)].copy()\nval = df[int(0.8*n):int(0.9*n)].copy()\ntest = df[int(0.9*n):].copy()\n\n# Reset indices for each split\ntrain = train.reset_index(drop=True)\nval = val.reset_index(drop=True)\ntest = test.reset_index(drop=True)\n\n# Filter out empty pairs to avoid NaNs during training\ntrain = train[(train[\"labels\"].str.len() > 0) & \n              (train[\"Situation\"].str.len() > 0) & \n              (train[\"customer_utterance\"].str.len() > 0)].reset_index(drop=True)\nval = val[(val[\"labels\"].str.len() > 0) & \n          (val[\"Situation\"].str.len() > 0) & \n          (val[\"customer_utterance\"].str.len() > 0)].reset_index(drop=True)\ntest = test[(test[\"labels\"].str.len() > 0) & \n            (test[\"Situation\"].str.len() > 0) & \n            (test[\"customer_utterance\"].str.len() > 0)].reset_index(drop=True)\n\nprint(f\"✓ Train: {len(train)} ({len(train)/n*100:.1f}%)\")\nprint(f\"✓ Val: {len(val)} ({len(val)/n*100:.1f}%)\")\nprint(f\"✓ Test: {len(test)} ({len(test)/n*100:.1f}%)\")\n\n# ---------- Build vocabulary ONLY on training data ----------\nprint(\"\\n📚 Building vocabulary on TRAIN set only...\")\n\ndef tokenize(text):\n    \"\"\"Simple whitespace tokenization\"\"\"\n    return text.split()\n\nword_counts = Counter()\n\n# Count words only from training data (all three text fields)\nfor _, row in train.iterrows():\n    word_counts.update(tokenize(row['Situation']))\n    word_counts.update(tokenize(row['customer_utterance']))\n    word_counts.update(tokenize(row['labels']))\n\nprint(f\"✓ Total unique words in train: {len(word_counts)}\")\nprint(f\"✓ Total word occurrences: {sum(word_counts.values())}\")\n\n# Show frequency distribution\nfreq_1 = sum(1 for cnt in word_counts.values() if cnt == 1)\nfreq_2_5 = sum(1 for cnt in word_counts.values() if 2 <= cnt <= 5)\nfreq_6_plus = sum(1 for cnt in word_counts.values() if cnt > 5)\nprint(f\"  • Frequency = 1: {freq_1} words (singletons)\")\nprint(f\"  • Frequency 2-5: {freq_2_5} words\")\nprint(f\"  • Frequency > 5: {freq_6_plus} words\")\n\n# ---------- Create vocabulary with special tokens ----------\n# Start with special tokens (fixed IDs)\nvocab = dict(SPECIAL_TOKENS)\n\n# Add template tokens FIRST (critical - these appear in input template)\nTEMPLATE_TOKENS = [\"emotion\", \"situation\", \"customer\", \"agent\", \"|\", \":\"]\nfor tok in TEMPLATE_TOKENS:\n    if tok not in vocab:\n        vocab[tok] = len(vocab)\n\nprint(f\"✓ Added {len(TEMPLATE_TOKENS)} template tokens (emotion, situation, customer, agent, |, :)\")\n\n# Add emotion-specific tokens (replace spaces with underscores for token names)\nunique_emotions = sorted(train['emotion'].unique())\nemotion_tokens = [f'<emotion_{emotion.replace(\" \", \"_\")}>' for emotion in unique_emotions]\n\nfor emotion_tok in emotion_tokens:\n    vocab[emotion_tok] = len(vocab)\n\nprint(f\"✓ Added {len(emotion_tokens)} emotion tokens\")\n\n# Add words from training data (with frequency threshold)\nwords_added = 0\nwords_filtered = 0\nfor word in sorted(word_counts.keys()):\n    if word not in vocab:\n        if word_counts[word] >= MIN_WORD_FREQ:\n            vocab[word] = len(vocab)\n            words_added += 1\n        else:\n            words_filtered += 1\n\nprint(f\"✓ Added {words_added} word tokens (freq >= {MIN_WORD_FREQ})\")\nprint(f\"✓ Filtered {words_filtered} rare words (freq < {MIN_WORD_FREQ}, will map to <unk>)\")\n\n# Create reverse mapping\nidx2word = {i: w for w, i in vocab.items()}\n\nprint(f\"\\n📊 Vocabulary statistics:\")\nprint(f\"  • Total vocab size: {len(vocab)}\")\nprint(f\"  • Special tokens: {len(SPECIAL_TOKENS)}\")\nprint(f\"  • Template tokens: {len(TEMPLATE_TOKENS)}\")\nprint(f\"  • Emotion tokens: {len(emotion_tokens)}\")\nprint(f\"  • Word tokens: {words_added}\")\nprint(f\"  • Rare words (filtered to <unk>): {words_filtered}\")\n\n# ---------- Create input templates and convert to IDs ----------\nprint(\"\\n🔧 Creating input templates and tokenizing to IDs...\")\n\ndef create_input_template(emotion, situation, customer_utterance):\n    \"\"\"\n    Create input template EXACTLY as specified in Task 2 assignment:\n    emotion: {emotion} | situation: {situation} | customer: {utterance} | agent:\n    \n    Using emotion token for the emotion value, with exact literal markers per spec.\n    \"\"\"\n    # Use emotion token for the emotion value\n    emo_tok = f'<emotion_{emotion.replace(\" \", \"_\")}>'\n    \n    # Build template with exact literal markers as per assignment spec\n    # Format: emotion: {emo_tok} | situation: {situation} | customer: {utterance} | agent:\n    template = f\"emotion : {emo_tok} | situation : {situation} | customer : {customer_utterance} | agent :\"\n    return template\n\ndef text_to_ids(text, vocab, add_bos=False, add_eos=False):\n    \"\"\"Convert text to list of token IDs, with <unk> for OOV words\"\"\"\n    tokens = tokenize(text)\n    ids = []\n    \n    if add_bos:\n        ids.append(SPECIAL_TOKENS['<bos>'])\n    \n    for token in tokens:\n        ids.append(vocab.get(token, SPECIAL_TOKENS['<unk>']))\n    \n    if add_eos:\n        ids.append(SPECIAL_TOKENS['<eos>'])\n    \n    return ids\n\ndef prepare_sequences(df_split, split_name):\n    \"\"\"Prepare source and target ID sequences for a dataset split\"\"\"\n    sequences = []\n    \n    for idx, row in df_split.iterrows():\n        # Create source: emotion + situation + customer utterance\n        src_text = create_input_template(\n            row['emotion'],\n            row['Situation'],\n            row['customer_utterance']\n        )\n        \n        # Target: agent response (must have <bos> at start and <eos> at end)\n        tgt_text = row['labels']\n        \n        # Convert to IDs\n        src_ids = text_to_ids(src_text, vocab, add_bos=False, add_eos=False)\n        tgt_ids = text_to_ids(tgt_text, vocab, add_bos=True, add_eos=True)  # <bos> ... <eos>\n        \n        # Apply sequence length cap AFTER templating\n        if len(src_ids) > MAX_SEQ_LENGTH:\n            src_ids = src_ids[:MAX_SEQ_LENGTH]\n        \n        if len(tgt_ids) > MAX_SEQ_LENGTH:\n            # Keep <bos> at start and <eos> at end, truncate middle\n            tgt_ids = [tgt_ids[0]] + tgt_ids[1:MAX_SEQ_LENGTH-1] + [tgt_ids[-1]]\n        \n        sequences.append({\n            'src_ids': src_ids,\n            'tgt_ids': tgt_ids,\n            'emotion': row['emotion'],\n            'situation': row['Situation'],\n            'customer_utterance': row['customer_utterance'],\n            'response': row['labels']\n        })\n    \n    print(f\"  ✓ {split_name}: {len(sequences)} sequences\")\n    return sequences\n\n# Prepare sequences for all splits\ntrain_sequences = prepare_sequences(train, 'Train')\nval_sequences = prepare_sequences(val, 'Val')\ntest_sequences = prepare_sequences(test, 'Test')\n\n# ---------- Sanity checks ----------\nprint(\"\\n🔍 Sanity checks on preprocessed sequences...\")\n\n# Check template tokens are in vocab (not mapping to <unk>)\nprint(\"  Template tokens in vocab:\")\nfor tok in TEMPLATE_TOKENS:\n    print(f\"    '{tok}' -> ID {vocab.get(tok, 'MISSING!')}\")\n\n# Check a few source sequences contain template tokens\nprint(\"\\n  Checking first 3 sources for template tokens:\")\nfor i in range(min(3, len(train_sequences))):\n    s = train_sequences[i][\"src_ids\"]\n    print(f\"    Source {i}:\")\n    print(f\"      has 'emotion' token? {vocab.get('emotion') in s}\")\n    print(f\"      has ':' token?       {vocab.get(':') in s}\")\n    print(f\"      has '|' token?       {vocab.get('|') in s}\")\n    print(f\"      has 'agent' token?   {vocab.get('agent') in s}\")\n\n# Check target format\nprint(\"\\n  Target format validation:\")\nprint(f\"    Example target starts with <bos> (1)? {train_sequences[0]['tgt_ids'][0] == SPECIAL_TOKENS['<bos>']}\")\nprint(f\"    Example target ends with <eos> (2)?   {train_sequences[0]['tgt_ids'][-1] == SPECIAL_TOKENS['<eos>']}\")\n\n# Check for any all-UNK sequences (would indicate vocab issues)\nunk_id = SPECIAL_TOKENS['<unk>']\nsrc_all_unk = sum(1 for seq in train_sequences if all(id == unk_id for id in seq['src_ids']))\ntgt_all_unk = sum(1 for seq in train_sequences if all(id == unk_id for id in seq['tgt_ids'][1:-1]))  # Skip BOS/EOS\nprint(f\"\\n  Sequences with all <unk> tokens:\")\nprint(f\"    Source: {src_all_unk} (should be 0)\")\nprint(f\"    Target: {tgt_all_unk} (should be 0)\")\n\nif src_all_unk > 0 or tgt_all_unk > 0:\n    print(\"    ⚠️  WARNING: Some sequences are all <unk>! Check vocab building.\")\n\n# Check emotion coverage across splits\nprint(\"\\n  Emotion coverage across splits:\")\nfor split_name, split_df in [(\"Train\", train), (\"Val\", val), (\"Test\", test)]:\n    unique_emos = split_df[\"emotion\"].nunique()\n    print(f\"    {split_name}: {unique_emos} unique emotions, {len(split_df)} samples\")\n\n# ---------- Save preprocessed data ----------\nprint(\"\\n💾 Saving preprocessed data...\")\nos.makedirs(output_dir, exist_ok=True)\n\n# Save CSV splits (with all columns for reference)\ntrain.to_csv(os.path.join(output_dir, 'train.csv'), index=False)\nval.to_csv(os.path.join(output_dir, 'val.csv'), index=False)\ntest.to_csv(os.path.join(output_dir, 'test.csv'), index=False)\nprint(\"  ✓ Saved train.csv, val.csv, test.csv\")\n\n# Save vocabulary\nwith open(os.path.join(output_dir, 'vocab.json'), 'w', encoding='utf-8') as f:\n    json.dump(vocab, f, ensure_ascii=False, indent=2)\n\nwith open(os.path.join(output_dir, 'idx2word.json'), 'w', encoding='utf-8') as f:\n    json.dump(idx2word, f, ensure_ascii=False, indent=2)\nprint(\"  ✓ Saved vocab.json, idx2word.json\")\n\n# Save special tokens for reference\nwith open(os.path.join(output_dir, 'special_tokens.json'), 'w', encoding='utf-8') as f:\n    json.dump({\n        'special_tokens': SPECIAL_TOKENS,\n        'template_tokens': {tok: vocab[tok] for tok in TEMPLATE_TOKENS},\n        'emotion_tokens': {tok: vocab[tok] for tok in emotion_tokens},\n        'max_seq_length': MAX_SEQ_LENGTH,\n        'min_word_freq': MIN_WORD_FREQ\n    }, f, ensure_ascii=False, indent=2)\nprint(\"  ✓ Saved special_tokens.json\")\n\n# Save word frequencies for analysis/reporting\nwith open(os.path.join(output_dir, 'word_freq_train.json'), 'w', encoding='utf-8') as f:\n    json.dump(dict(word_counts.most_common(1000)), f, ensure_ascii=False, indent=2)\nprint(\"  ✓ Saved word_freq_train.json (top 1000 words)\")\n\n# Save tokenized ID sequences (JSONL format for Task 4)\ndef save_ids_jsonl(sequences, filepath):\n    \"\"\"Save sequences in JSONL format\"\"\"\n    with open(filepath, 'w', encoding='utf-8') as f:\n        for seq in sequences:\n            json.dump({\n                'src_ids': seq['src_ids'],\n                'tgt_ids': seq['tgt_ids']\n            }, f, ensure_ascii=False)\n            f.write('\\n')\n\nsave_ids_jsonl(train_sequences, os.path.join(output_dir, 'train_ids.jsonl'))\nsave_ids_jsonl(val_sequences, os.path.join(output_dir, 'val_ids.jsonl'))\nsave_ids_jsonl(test_sequences, os.path.join(output_dir, 'test_ids.jsonl'))\nprint(\"  ✓ Saved train_ids.jsonl, val_ids.jsonl, test_ids.jsonl\")\n\n# Also save human-readable pair files for inspection\ndef save_pairs_csv(sequences, df_split, filepath):\n    \"\"\"Save source-target pairs in CSV format for easy inspection\"\"\"\n    pairs_df = pd.DataFrame({\n        'emotion': df_split['emotion'].values,\n        'situation': df_split['Situation'].values,\n        'customer_utterance': df_split['customer_utterance'].values,\n        'agent_response': df_split['labels'].values,\n        'src_length': [len(seq['src_ids']) for seq in sequences],\n        'tgt_length': [len(seq['tgt_ids']) for seq in sequences]\n    })\n    pairs_df.to_csv(filepath, index=False)\n\nsave_pairs_csv(train_sequences, train, os.path.join(output_dir, 'train_pairs.csv'))\nsave_pairs_csv(val_sequences, val, os.path.join(output_dir, 'val_pairs.csv'))\nsave_pairs_csv(test_sequences, test, os.path.join(output_dir, 'test_pairs.csv'))\nprint(\"  ✓ Saved train_pairs.csv, val_pairs.csv, test_pairs.csv\")\n\n# ---------- Statistics ----------\nprint(\"\\n\" + \"=\"*70)\nprint(\"✅ PREPROCESSING COMPLETE\")\nprint(\"=\"*70)\n\nprint(\"\\n📁 Dataset splits saved:\")\nprint(f\"  • train.csv ({len(train)} samples)\")\nprint(f\"  • val.csv ({len(val)} samples)\")\nprint(f\"  • test.csv ({len(test)} samples)\")\n\nprint(\"\\n📝 Tokenized ID files saved (for Task 4):\")\nprint(f\"  • train_ids.jsonl ({len(train_sequences)} sequences)\")\nprint(f\"  • val_ids.jsonl ({len(val_sequences)} sequences)\")\nprint(f\"  • test_ids.jsonl ({len(test_sequences)} sequences)\")\n\nprint(\"\\n📋 Pair files saved (for inspection):\")\nprint(f\"  • train_pairs.csv, val_pairs.csv, test_pairs.csv\")\n\nprint(\"\\n📚 Vocabulary files saved:\")\nprint(f\"  • vocab.json (word → id mapping, {len(vocab)} entries)\")\nprint(f\"  • idx2word.json (id → word mapping, {len(idx2word)} entries)\")\nprint(f\"  • special_tokens.json (special token definitions)\")\n\nprint(\"\\n🔑 Special tokens:\")\nfor token, idx in SPECIAL_TOKENS.items():\n    print(f\"  {token}: {idx}\")\n\nprint(f\"\\n📝 Template tokens ({len(TEMPLATE_TOKENS)} total):\")\nfor tok in TEMPLATE_TOKENS:\n    print(f\"  '{tok}': {vocab[tok]}\")\n\nprint(f\"\\n🎭 Emotion tokens ({len(emotion_tokens)} total):\")\nfor tok in emotion_tokens[:10]:\n    print(f\"  {tok}: {vocab[tok]}\")\nif len(emotion_tokens) > 10:\n    print(f\"  ... and {len(emotion_tokens) - 10} more\")\n\nprint(f\"\\n📏 Max sequence length: {MAX_SEQ_LENGTH} tokens (applied after templating)\")\n\n# Sample statistics\navg_src_len = sum(len(seq['src_ids']) for seq in train_sequences) / len(train_sequences)\navg_tgt_len = sum(len(seq['tgt_ids']) for seq in train_sequences) / len(train_sequences)\navg_situation_len = train['Situation'].apply(lambda x: len(x.split())).mean()\navg_utterance_len = train['customer_utterance'].apply(lambda x: len(x.split())).mean()\navg_response_len = train['labels'].apply(lambda x: len(x.split())).mean()\n\n# Count sequences that hit the cap\nsrc_capped = sum(1 for seq in train_sequences if len(seq['src_ids']) == MAX_SEQ_LENGTH)\ntgt_capped = sum(1 for seq in train_sequences if len(seq['tgt_ids']) == MAX_SEQ_LENGTH)\n\nprint(f\"\\n📊 Average lengths (train set):\")\nprint(f\"  • Situation: {avg_situation_len:.1f} tokens\")\nprint(f\"  • Customer utterance: {avg_utterance_len:.1f} tokens\")\nprint(f\"  • Agent response: {avg_response_len:.1f} tokens\")\nprint(f\"  • Source (full input template): {avg_src_len:.1f} IDs\")\nprint(f\"  • Target (with <bos> and <eos>): {avg_tgt_len:.1f} IDs\")\nprint(f\"  • Sequences capped at {MAX_SEQ_LENGTH}: src={src_capped}, tgt={tgt_capped}\")\n\nprint(\"\\n🔍 Example preprocessed sample:\")\nsample = train_sequences[0]\nprint(f\"  Emotion: {sample['emotion']}\")\nprint(f\"  Situation: {sample['situation'][:80]}...\")\nprint(f\"  Customer: {sample['customer_utterance'][:80]}...\")\nprint(f\"  Response: {sample['response'][:80]}...\")\nprint(f\"  Source length: {len(sample['src_ids'])} IDs\")\nprint(f\"  Target length: {len(sample['tgt_ids'])} IDs (with <bos> and <eos>)\")\nprint(f\"  Source IDs (first 20): {sample['src_ids'][:20]}\")\nprint(f\"  Target IDs (first 10): {sample['tgt_ids'][:10]}\")\nprint(f\"  Target starts with <bos> (1)? {sample['tgt_ids'][0] == 1}\")\nprint(f\"  Target ends with <eos> (2)? {sample['tgt_ids'][-1] == 2}\")\n\nprint(\"\\n🔍 Input template format (Task 2 spec):\")\nsample_template = create_input_template(\n    train.iloc[0]['emotion'],\n    train.iloc[0]['Situation'][:40] + \"...\",\n    train.iloc[0]['customer_utterance'][:40] + \"...\"\n)\nprint(f\"  {sample_template[:120]}...\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"✨ Ready for Task 2: Sequence-to-Sequence Modeling\")\nprint(\"=\"*70) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:13:23.488661Z","iopub.execute_input":"2025-10-17T17:13:23.489441Z","iopub.status.idle":"2025-10-17T17:13:42.224515Z","shell.execute_reply.started":"2025-10-17T17:13:23.489416Z","shell.execute_reply":"2025-10-17T17:13:42.223868Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nTASK 1: PREPROCESSING FOR EMPATHETIC DIALOGUES\n======================================================================\n✓ Required modules loaded: pandas, re, json, unicodedata\n\n📂 Loading dataset...\n✓ Data loaded: 64636 samples\n✓ Columns: ['Unnamed: 0', 'Situation', 'emotion', 'empathetic_dialogues', 'labels', 'Unnamed: 5', 'Unnamed: 6']\n✓ After removing NaN: 64632 samples\n\n🔍 Parsing empathetic dialogues...\n✓ Extracted customer utterances\n✓ Sample: 'I remember going to see the fireworks with my best friend. It was the first time...'\n✓ After filtering empty utterances: 64591 samples\n\n🧹 Applying hard normalization...\n✓ After filtering emotions: 64591 samples\n✓ Unique emotions: 32\n\n📊 Splitting dataset...\n✓ Train: 51672 (80.0%)\n✓ Val: 6459 (10.0%)\n✓ Test: 6460 (10.0%)\n\n📚 Building vocabulary on TRAIN set only...\n✓ Total unique words in train: 21286\n✓ Total word occurrences: 2554215\n  • Frequency = 1: 4364 words (singletons)\n  • Frequency 2-5: 8592 words\n  • Frequency > 5: 8330 words\n✓ Added 6 template tokens (emotion, situation, customer, agent, |, :)\n✓ Added 32 emotion tokens\n✓ Added 16916 word tokens (freq >= 2)\n✓ Filtered 4364 rare words (freq < 2, will map to <unk>)\n\n📊 Vocabulary statistics:\n  • Total vocab size: 16958\n  • Special tokens: 4\n  • Template tokens: 6\n  • Emotion tokens: 32\n  • Word tokens: 16916\n  • Rare words (filtered to <unk>): 4364\n\n🔧 Creating input templates and tokenizing to IDs...\n  ✓ Train: 51672 sequences\n  ✓ Val: 6459 sequences\n  ✓ Test: 6460 sequences\n\n🔍 Sanity checks on preprocessed sequences...\n  Template tokens in vocab:\n    'emotion' -> ID 4\n    'situation' -> ID 5\n    'customer' -> ID 6\n    'agent' -> ID 7\n    '|' -> ID 8\n    ':' -> ID 9\n\n  Checking first 3 sources for template tokens:\n    Source 0:\n      has 'emotion' token? True\n      has ':' token?       True\n      has '|' token?       True\n      has 'agent' token?   True\n    Source 1:\n      has 'emotion' token? True\n      has ':' token?       True\n      has '|' token?       True\n      has 'agent' token?   True\n    Source 2:\n      has 'emotion' token? True\n      has ':' token?       True\n      has '|' token?       True\n      has 'agent' token?   True\n\n  Target format validation:\n    Example target starts with <bos> (1)? True\n    Example target ends with <eos> (2)?   True\n\n  Sequences with all <unk> tokens:\n    Source: 0 (should be 0)\n    Target: 7 (should be 0)\n    ⚠️  WARNING: Some sequences are all <unk>! Check vocab building.\n\n  Emotion coverage across splits:\n    Train: 32 unique emotions, 51672 samples\n    Val: 32 unique emotions, 6459 samples\n    Test: 32 unique emotions, 6460 samples\n\n💾 Saving preprocessed data...\n  ✓ Saved train.csv, val.csv, test.csv\n  ✓ Saved vocab.json, idx2word.json\n  ✓ Saved special_tokens.json\n  ✓ Saved word_freq_train.json (top 1000 words)\n  ✓ Saved train_ids.jsonl, val_ids.jsonl, test_ids.jsonl\n  ✓ Saved train_pairs.csv, val_pairs.csv, test_pairs.csv\n\n======================================================================\n✅ PREPROCESSING COMPLETE\n======================================================================\n\n📁 Dataset splits saved:\n  • train.csv (51672 samples)\n  • val.csv (6459 samples)\n  • test.csv (6460 samples)\n\n📝 Tokenized ID files saved (for Task 4):\n  • train_ids.jsonl (51672 sequences)\n  • val_ids.jsonl (6459 sequences)\n  • test_ids.jsonl (6460 sequences)\n\n📋 Pair files saved (for inspection):\n  • train_pairs.csv, val_pairs.csv, test_pairs.csv\n\n📚 Vocabulary files saved:\n  • vocab.json (word → id mapping, 16958 entries)\n  • idx2word.json (id → word mapping, 16958 entries)\n  • special_tokens.json (special token definitions)\n\n🔑 Special tokens:\n  <pad>: 0\n  <bos>: 1\n  <eos>: 2\n  <unk>: 3\n\n📝 Template tokens (6 total):\n  'emotion': 4\n  'situation': 5\n  'customer': 6\n  'agent': 7\n  '|': 8\n  ':': 9\n\n🎭 Emotion tokens (32 total):\n  <emotion_afraid>: 10\n  <emotion_angry>: 11\n  <emotion_annoyed>: 12\n  <emotion_anticipating>: 13\n  <emotion_anxious>: 14\n  <emotion_apprehensive>: 15\n  <emotion_ashamed>: 16\n  <emotion_caring>: 17\n  <emotion_confident>: 18\n  <emotion_content>: 19\n  ... and 22 more\n\n📏 Max sequence length: 128 tokens (applied after templating)\n\n📊 Average lengths (train set):\n  • Situation: 19.5 tokens\n  • Customer utterance: 15.5 tokens\n  • Agent response: 14.4 tokens\n  • Source (full input template): 47.0 IDs\n  • Target (with <bos> and <eos>): 16.4 IDs\n  • Sequences capped at 128: src=182, tgt=0\n\n🔍 Example preprocessed sample:\n  Emotion: afraid\n  Situation: i am really not liking mondays ....\n  Customer: i am really not liking mondays ....\n  Response: yeah , they're sometimes hard starts to the week . did anything bad happen to yo...\n  Source length: 26 IDs\n  Target length: 34 IDs (with <bos> and <eos>)\n  Source IDs (first 20): [4, 9, 10, 8, 5, 9, 7751, 1176, 12047, 10210, 8892, 9718, 351, 8, 6, 9, 7751, 1176, 12047, 10210]\n  Target IDs (first 10): [1, 16839, 344, 15043, 13850, 7209, 14168, 15217, 14998, 16392]\n  Target starts with <bos> (1)? True\n  Target ends with <eos> (2)? True\n\n🔍 Input template format (Task 2 spec):\n  emotion : <emotion_afraid> | situation : i am really not liking mondays .... | customer : i am really not liking mondays...\n\n======================================================================\n✨ Ready for Task 2: Sequence-to-Sequence Modeling\n======================================================================\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# **TASK 2 - Input and Output Definition**","metadata":{}},{"cell_type":"code","source":"# ===== Task-2: exact X/Y format (with checks) =====\nimport os, json, re, pandas as pd\n\ndata_dir = \"/kaggle/working\" if os.path.exists(\"/kaggle/input\") else \".\"\nwith open(f\"{data_dir}/vocab.json\",\"r\") as f: stoi = json.load(f)\n\nPAD, BOS, EOS, UNK = stoi[\"<pad>\"], stoi[\"<bos>\"], stoi[\"<eos>\"], stoi[\"<unk>\"]\n\nCUSTOMER_RE = re.compile(r\"customer\\s*:?\\s*(.*?)(?:\\s*agent\\s*:|$)\", re.IGNORECASE)\ndef extract_customer_text(text: str) -> str:\n    if pd.isna(text): return \"\"\n    m = CUSTOMER_RE.search(str(text))\n    return m.group(1).strip() if m else str(text).strip()\n\ndef tok(s): return str(s).split()\ndef enc(tokens): return [stoi.get(t, UNK) for t in tokens]\n\ndef pick_col(df, candidates, default=None):\n    cmap = {c.lower(): c for c in df.columns}\n    for name in candidates:\n        if name.lower() in cmap:\n            return cmap[name.lower()]\n    return default\n\ndef make_pairs(in_csv, out_csv, out_jsonl):\n    df_raw = pd.read_csv(in_csv)\n\n    emotion_c   = pick_col(df_raw, [\"emotion\",\"feeling\"])\n    situation_c = pick_col(df_raw, [\"situation\",\"context\",\"prompt\",\"situation_text\",\"Situation\"])\n    utter_c     = pick_col(df_raw, [\"utterance\",\"text\",\"customer\",\"message\",\"empathetic_dialogues\"])\n    response_c  = pick_col(df_raw, [\"response\",\"reply\",\"agent_reply\",\"target\",\"reference\",\"output\",\"gold\",\"labels\"])\n\n    if utter_c is None or response_c is None:\n        raise ValueError(f\"{in_csv}: missing utterance/response-like columns. Have={list(df_raw.columns)}\")\n\n    utterance_series = (\n        df_raw[utter_c].apply(extract_customer_text)\n        if utter_c else pd.Series([\"\"] * len(df_raw))\n    )\n\n    df = pd.DataFrame({\n        \"emotion\":   df_raw[emotion_c]   if emotion_c   else \"\",\n        \"situation\": df_raw[situation_c] if situation_c else \"\",\n        \"utterance\": utterance_series,\n        \"response\":  df_raw[response_c]\n    }).fillna(\"\")\n\n    # exact X format\n    df[\"input_text\"] = (\n        \"Emotion: \" + df[\"emotion\"].astype(str).str.strip()\n        + \" | Situation: \" + df[\"situation\"].astype(str).str.strip()\n        + \" | Customer: \" + df[\"utterance\"].astype(str).str.strip()\n        + \" Agent:\"\n    )\n    df[\"target_text\"] = df[\"response\"].astype(str).str.strip()\n\n    # drop empty targets\n    df = df[df[\"target_text\"].ne(\"\")].reset_index(drop=True)\n\n    # save readable pairs\n    df[[\"input_text\",\"target_text\"]].to_csv(out_csv, index=False)\n\n    # save ids\n    with open(out_jsonl, \"w\") as f:\n        for _, r in df.iterrows():\n            src_ids = enc(tok(r[\"input_text\"]))\n            tgt_ids = [BOS] + enc(tok(r[\"target_text\"])) + [EOS]\n            f.write(json.dumps({\"src_ids\": src_ids, \"tgt_ids\": tgt_ids}) + \"\\n\")\n\n    # sanity vs. brief (non-failing informative prints)\n    ex1_x = \"Emotion: sentimental | Situation: I remember going to the fireworks with my best friend... | Customer: This was a best friend. I miss her. Agent:\"\n    ex1_y = \"Where has she gone?\"\n    ex2_x = \"Emotion: afraid | Situation: I used to scare for darkness | Customer: it feels like hitting to blank wall when I see the darkness Agent:\"\n    ex2_y = \"Oh ya? I don't really see how\"\n    print(\"Format check (examples from brief):\")\n    print(\"X ex1 ->\", ex1_x[:110] + \" ...\")\n    print(\"Y ex1 ->\", ex1_y)\n    print(\"X ex2 ->\", ex2_x[:110] + \" ...\")\n    print(\"Y ex2 ->\", ex2_y)\n\nfor name in (\"train\",\"val\",\"test\"):\n    make_pairs(f\"{data_dir}/{name}.csv\",\n               f\"{data_dir}/{name}_pairs.csv\",\n               f\"{data_dir}/{name}_ids.jsonl\")\n\nprint(\"✅ Task 2 aligned with spec — exact X format + IDs written.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:13:42.225689Z","iopub.execute_input":"2025-10-17T17:13:42.225889Z","iopub.status.idle":"2025-10-17T17:13:47.992963Z","shell.execute_reply.started":"2025-10-17T17:13:42.225874Z","shell.execute_reply":"2025-10-17T17:13:47.992208Z"}},"outputs":[{"name":"stdout","text":"Format check (examples from brief):\nX ex1 -> Emotion: sentimental | Situation: I remember going to the fireworks with my best friend... | Customer: This wa ...\nY ex1 -> Where has she gone?\nX ex2 -> Emotion: afraid | Situation: I used to scare for darkness | Customer: it feels like hitting to blank wall when ...\nY ex2 -> Oh ya? I don't really see how\nFormat check (examples from brief):\nX ex1 -> Emotion: sentimental | Situation: I remember going to the fireworks with my best friend... | Customer: This wa ...\nY ex1 -> Where has she gone?\nX ex2 -> Emotion: afraid | Situation: I used to scare for darkness | Customer: it feels like hitting to blank wall when ...\nY ex2 -> Oh ya? I don't really see how\nFormat check (examples from brief):\nX ex1 -> Emotion: sentimental | Situation: I remember going to the fireworks with my best friend... | Customer: This wa ...\nY ex1 -> Where has she gone?\nX ex2 -> Emotion: afraid | Situation: I used to scare for darkness | Customer: it feels like hitting to blank wall when ...\nY ex2 -> Oh ya? I don't really see how\n✅ Task 2 aligned with spec — exact X format + IDs written.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# **TASK 3 — Transformer Encoder–Decoder Model**","metadata":{}},{"cell_type":"code","source":"# ===== Task-3: spec-aligned Transformer (distinct layers + proper masks) =====\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Positional Encoding\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))  # [1,L,D]\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1), :]\n\n# Helper to guard against all-False mask rows (can cause softmax NaNs)\ndef ensure_nonempty_rows(mask):\n    \"\"\"Ensure each row has at least one True value to prevent softmax NaNs.\"\"\"\n    row_has_true = mask.any(dim=-1, keepdim=True)\n    safe = mask.clone()\n    safe[:, :, 0] |= ~row_has_true.squeeze(-1)\n    return safe\n\n# Multi-Head Attention that accepts boolean (preferred) or additive masks\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, n_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % n_heads == 0\n        self.h = n_heads\n        self.dk = d_model // n_heads\n        self.Wq, self.Wk, self.Wv = nn.Linear(d_model, d_model), nn.Linear(d_model, d_model), nn.Linear(d_model, d_model)\n        self.Wo = nn.Linear(d_model, d_model)\n        self.drop = nn.Dropout(dropout)\n        self.attn_drop = nn.Dropout(dropout)  # Dropout on attention weights\n    def _split(self, x):\n        B,L,D = x.size()\n        return x.view(B, L, self.h, self.dk).transpose(1, 2)  # [B,h,L,dk]\n    def _merge(self, x):\n        B,h,L,dk = x.size()\n        return x.transpose(1, 2).contiguous().view(B, L, h*dk)\n    def forward(self, q, k, v, mask=None):\n        Q, K, V = self._split(self.Wq(q)), self._split(self.Wk(k)), self._split(self.Wv(v))\n        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.dk)  # [B,h,Lq,Lk]\n        if mask is not None:\n            if mask.dtype == torch.bool:  # True = keep, False = mask\n                mask = ensure_nonempty_rows(mask)  # Guard against all-False rows\n                scores = scores.masked_fill(~mask.unsqueeze(1), float(\"-inf\"))\n            else:  # additive mask (0 or -inf)\n                scores = scores + mask.unsqueeze(1)\n        attn = F.softmax(scores, dim=-1)\n        attn = torch.nan_to_num(attn, nan=0.0)  # Safety fallback for any NaNs\n        attn = self.attn_drop(attn)  # Dropout on attention weights\n        out = attn @ V\n        out = self._merge(out)\n        return self.Wo(self.drop(out))\n\n# FFN\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.ff = nn.Sequential(\n            nn.Linear(d_model, d_ff), nn.ReLU(), nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model), nn.Dropout(dropout)\n        )\n    def forward(self, x): return self.ff(x)\n\n# Encoder/Decoder layers\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, n_heads, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.sa = MultiHeadAttention(d_model, n_heads, dropout)\n        self.ff = FeedForward(d_model, d_ff, dropout)\n        self.n1, self.n2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n    def forward(self, x, attn_mask=None):\n        x = self.n1(x + self.sa(x, x, x, attn_mask))\n        x = self.n2(x + self.ff(x))\n        return x\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, n_heads, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.msa = MultiHeadAttention(d_model, n_heads, dropout)\n        self.xattn = MultiHeadAttention(d_model, n_heads, dropout)\n        self.ff = FeedForward(d_model, d_ff, dropout)\n        self.n1, self.n2, self.n3 = nn.LayerNorm(d_model), nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n    def forward(self, x, mem, self_mask=None, mem_mask=None):\n        x = self.n1(x + self.msa(x, x, x, self_mask))\n        x = self.n2(x + self.xattn(x, mem, mem, mem_mask))\n        x = self.n3(x + self.ff(x))\n        return x\n\n# Stacks with DISTINCT layer instances\nclass Encoder(nn.Module):\n    def __init__(self, d_model, n_heads, num_layers, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n    def forward(self, x, attn_mask=None):\n        for lyr in self.layers:\n            x = lyr(x, attn_mask)\n        return x\n\nclass Decoder(nn.Module):\n    def __init__(self, d_model, n_heads, num_layers, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n    def forward(self, x, mem, self_mask=None, mem_mask=None):\n        for lyr in self.layers:\n            x = lyr(x, mem, self_mask, mem_mask)\n        return x\n\n# Mask helpers\ndef make_pad_mask(ids, pad_idx):\n    return (ids != pad_idx)  # [B,L] True=token, False=pad\n\ndef make_attn_mask(valid_q, valid_k):\n    return valid_q.unsqueeze(2) & valid_k.unsqueeze(1)  # [B,Lq,Lk] bool\n\ndef make_causal_mask(L, device):\n    return torch.tril(torch.ones(L, L, dtype=torch.bool, device=device))  # [L,L]\n\n# Full model\nclass TransformerModel(nn.Module):\n    def __init__(self, vocab_size, d_model=256, n_heads=2, num_layers=2, d_ff=2048, dropout=0.1, pad_idx=0, weight_tying=True):\n        super().__init__()\n        self.pad_idx = pad_idx\n        self.d_model = d_model\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n        self.pos = PositionalEncoding(d_model)\n        self.embed_drop = nn.Dropout(dropout)  # Dropout on embeddings\n        self.encoder = Encoder(d_model, n_heads, num_layers, d_ff, dropout)\n        self.decoder = Decoder(d_model, n_heads, num_layers, d_ff, dropout)\n        self.out = nn.Linear(d_model, vocab_size)\n        \n        # Weight tying: share embeddings with output layer\n        if weight_tying:\n            self.out.weight = self.embed.weight\n\n    def forward(self, src, tgt):\n        device = src.device\n        B, Ls = src.size()\n        Lt = tgt.size(1)\n\n        # masks (boolean)\n        src_valid = make_pad_mask(src, self.pad_idx)          # [B,Ls]\n        tgt_valid = make_pad_mask(tgt, self.pad_idx)          # [B,Lt]\n        enc_self  = make_attn_mask(src_valid, src_valid)      # [B,Ls,Ls]\n        dec_caus  = make_causal_mask(Lt, device)              # [Lt,Lt]\n        dec_self  = dec_caus.unsqueeze(0).expand(B, -1, -1) & make_attn_mask(tgt_valid, tgt_valid)  # [B,Lt,Lt]\n        cross     = make_attn_mask(tgt_valid, src_valid)      # [B,Lt,Ls]\n\n        # embeddings + PE + dropout\n        src_e = self.embed_drop(self.pos(self.embed(src) * math.sqrt(self.d_model)))\n        tgt_e = self.embed_drop(self.pos(self.embed(tgt) * math.sqrt(self.d_model)))\n\n        mem   = self.encoder(src_e, enc_self)\n        dec   = self.decoder(tgt_e, mem, dec_self, cross)\n        return self.out(dec)\n\n# quick smoke test\nif __name__ == \"__main__\":\n    torch.manual_seed(0)\n    V, PAD = 10000, 0\n    m = TransformerModel(vocab_size=V, d_model=256, n_heads=2, num_layers=2, dropout=0.1, pad_idx=PAD, weight_tying=True)\n    src = torch.randint(1, V, (2, 10)); src[:, -2:] = PAD\n    tgt = torch.randint(1, V, (2, 9));  tgt[:, -1]  = PAD\n    logits = m(src, tgt)\n    print(\"Shape:\", logits.shape)  # (2, 9, 10000)\n    print(\"✅ Task 3 complete – spec-aligned Transformer with NaN guards, attention/embedding dropout, and weight tying.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:13:47.993931Z","iopub.execute_input":"2025-10-17T17:13:47.994241Z","iopub.status.idle":"2025-10-17T17:13:48.125372Z","shell.execute_reply.started":"2025-10-17T17:13:47.994217Z","shell.execute_reply":"2025-10-17T17:13:48.124581Z"}},"outputs":[{"name":"stdout","text":"Shape: torch.Size([2, 9, 10000])\n✅ Task 3 complete – spec-aligned Transformer with NaN guards, attention/embedding dropout, and weight tying.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# **Task 4 - Training & Hyperparameters**","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# TASK 4 — Training & Hyperparameters (final, Kaggle-ready)\n# ============================================================\n\nimport os, json, math, random\nimport numpy as np\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ------------------------ Config ----------------------------\nclass Config:\n    # ==== Assignment hyperparameters ====\n    batch_size = 32          # set 32 or 64\n    lr = 3e-4                # in [1e-4, 5e-4]\n    betas = (0.9, 0.98)      # Adam betas\n    epochs = 6\n    grad_clip = 1.0\n\n    # ==== Model ====\n    d_model = 256\n    n_heads = 2\n    num_layers = 2\n    d_ff = 2048\n    dropout = 0.1\n    max_seq_len = 128\n\n    # ==== Paths ====\n    data_dir = \"/kaggle/working\" if os.path.exists(\"/kaggle/input\") else \".\"\n    save_dir = \"checkpoints\"\n\n    # ==== Device ====\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef set_seed(seed=42):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\ncfg = Config()\n\n# -------------------- Dataset (JSONL) -----------------------\nclass EmpatheticDataset(Dataset):\n    \"\"\"\n    Expects JSONL lines like:\n      {\"src_ids\": [...], \"tgt_ids\": [...]}\n    Uses vocab.json for <pad>/<bos>/<eos> etc.\n    \"\"\"\n    def __init__(self, jsonl_path, vocab, max_len=128):\n        self.rows = []\n        self.vocab = vocab\n        self.max_len = max_len\n        self.pad = vocab[\"<pad>\"]\n\n        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                ex = json.loads(line)\n                s = ex[\"src_ids\"][:max_len]\n                t = ex[\"tgt_ids\"][:max_len]\n                self.rows.append((s, t))\n\n    def __len__(self): return len(self.rows)\n\n    def __getitem__(self, idx):\n        s, t = self.rows[idx]\n        return torch.tensor(s, dtype=torch.long), torch.tensor(t, dtype=torch.long)\n\n    def collate_fn(self, batch):\n        src_list, tgt_list = zip(*batch)\n        src = nn.utils.rnn.pad_sequence(src_list, batch_first=True, padding_value=self.pad)\n        tgt = nn.utils.rnn.pad_sequence(tgt_list, batch_first=True, padding_value=self.pad)\n        return src, tgt\n\n# ------------------- Transformer (from scratch) -------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))  # [1,L,D]\n    def forward(self, x):  # x: [B,L,D]\n        return x + self.pe[:, :x.size(1), :]\n\ndef make_pad_mask(ids, pad_idx):  # True=valid token\n    return (ids != pad_idx)\n\ndef make_attn_mask(valid_q, valid_k):  # [B,Lq,Lk]\n    return valid_q.unsqueeze(2) & valid_k.unsqueeze(1)\n\ndef make_causal_mask(L, device):      # [L,L]\n    return torch.tril(torch.ones(L, L, dtype=torch.bool, device=device))\n\ndef _ensure_nonempty_rows(mask):\n    # mask [B,Lq,Lk] (True=keep). Ensure each row has >=1 True to avoid NaN in softmax.\n    has_true = mask.any(dim=-1, keepdim=True)\n    mask = mask.clone()\n    mask[:, :, 0] = mask[:, :, 0] | (~has_true.squeeze(-1))\n    return mask\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, n_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % n_heads == 0\n        self.h = n_heads\n        self.dk = d_model // n_heads\n        self.Wq = nn.Linear(d_model, d_model)\n        self.Wk = nn.Linear(d_model, d_model)\n        self.Wv = nn.Linear(d_model, d_model)\n        self.Wo = nn.Linear(d_model, d_model)\n        self.drop = nn.Dropout(dropout)\n\n    def _split(self, x):  # [B,L,D] -> [B,h,L,dk]\n        B,L,D = x.size()\n        return x.view(B, L, self.h, self.dk).transpose(1,2)\n\n    def _merge(self, x):  # [B,h,L,dk] -> [B,L,D]\n        B,h,L,dk = x.size()\n        return x.transpose(1,2).contiguous().view(B,L,h*dk)\n\n    def forward(self, q, k, v, mask=None):\n        Q, K, V = self._split(self.Wq(q)), self._split(self.Wk(k)), self._split(self.Wv(v))\n        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.dk)  # [B,h,Lq,Lk]\n        if mask is not None:\n            if mask.dtype == torch.bool:\n                mask = _ensure_nonempty_rows(mask)\n                scores = scores.masked_fill(~mask.unsqueeze(1), float(\"-inf\"))\n            else:  # additive mask\n                scores = scores + mask.unsqueeze(1)\n        attn = F.softmax(scores, dim=-1)\n        attn = torch.nan_to_num(attn, nan=0.0)\n        out = attn @ V\n        out = self._merge(out)\n        return self.Wo(self.drop(out))\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.ff = nn.Sequential(\n            nn.Linear(d_model, d_ff), nn.ReLU(), nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model), nn.Dropout(dropout)\n        )\n    def forward(self, x): return self.ff(x)\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, n_heads, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.sa = MultiHeadAttention(d_model, n_heads, dropout)\n        self.ff = FeedForward(d_model, d_ff, dropout)\n        self.n1, self.n2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n    def forward(self, x, attn_mask=None):\n        x = self.n1(x + self.sa(x, x, x, attn_mask))\n        x = self.n2(x + self.ff(x))\n        return x\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, n_heads, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.msa = MultiHeadAttention(d_model, n_heads, dropout)\n        self.xattn = MultiHeadAttention(d_model, n_heads, dropout)\n        self.ff = FeedForward(d_model, d_ff, dropout)\n        self.n1, self.n2, self.n3 = nn.LayerNorm(d_model), nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n    def forward(self, x, mem, self_mask=None, mem_mask=None):\n        x = self.n1(x + self.msa(x, x, x, self_mask))\n        x = self.n2(x + self.xattn(x, mem, mem, mem_mask))\n        x = self.n3(x + self.ff(x))\n        return x\n\nclass Encoder(nn.Module):\n    def __init__(self, d_model, n_heads, num_layers, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n    def forward(self, x, attn_mask=None):\n        for lyr in self.layers: x = lyr(x, attn_mask)\n        return x\n\nclass Decoder(nn.Module):\n    def __init__(self, d_model, n_heads, num_layers, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n    def forward(self, x, mem, self_mask=None, mem_mask=None):\n        for lyr in self.layers: x = lyr(x, mem, self_mask, mem_mask)\n        return x\n\nclass TransformerModel(nn.Module):\n    def __init__(self, vocab_size, d_model=256, n_heads=2, num_layers=2, d_ff=2048, dropout=0.1, pad_idx=0):\n        super().__init__()\n        self.pad_idx = pad_idx\n        self.d_model = d_model\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n        self.pos = PositionalEncoding(d_model)\n        self.encoder = Encoder(d_model, n_heads, num_layers, d_ff, dropout)\n        self.decoder = Decoder(d_model, n_heads, num_layers, d_ff, dropout)\n        self.out = nn.Linear(d_model, vocab_size)\n\n    def forward(self, src, tgt):\n        device = src.device\n        B, Ls = src.size()\n        Lt = tgt.size(1)\n\n        src_valid = make_pad_mask(src, self.pad_idx)\n        tgt_valid = make_pad_mask(tgt, self.pad_idx)\n        enc_self  = make_attn_mask(src_valid, src_valid)                # [B,Ls,Ls]\n        dec_caus  = make_causal_mask(Lt, device)                        # [Lt,Lt]\n        dec_self  = dec_caus.unsqueeze(0).expand(B,-1,-1) & make_attn_mask(tgt_valid, tgt_valid)\n        cross     = make_attn_mask(tgt_valid, src_valid)                # [B,Lt,Ls]\n\n        src_e = self.pos(self.embed(src) * math.sqrt(self.d_model))\n        tgt_e = self.pos(self.embed(tgt) * math.sqrt(self.d_model))\n\n        mem = self.encoder(src_e, enc_self)\n        dec = self.decoder(tgt_e, mem, dec_self, cross)\n        return self.out(dec)\n\n# ------------------------ Inference --------------------------\ndef greedy_decode(model, src, vocab, max_len=128, device=None):\n    if device is None: device = src.device\n    model.eval()\n    bos, eos = vocab[\"<bos>\"], vocab[\"<eos>\"]\n    B = src.size(0)\n    tgt = torch.full((B,1), bos, dtype=torch.long, device=device)\n\n    with torch.no_grad():\n        for _ in range(max_len-1):\n            logits = model(src, tgt)              # [B,T,V]\n            next_tok = logits[:,-1,:].argmax(-1, keepdim=True)\n            tgt = torch.cat([tgt, next_tok], dim=1)\n            if (next_tok.squeeze(-1) == eos).all(): break\n    return tgt[:,1:]  # strip BOS\n\n# ------------------------ Metrics ---------------------------\ndef _install_metrics():\n    try:\n        import sacrebleu, rouge_score, evaluate  # noqa\n    except Exception:\n        import subprocess, sys\n        subprocess.check_call([sys.executable,\"-m\",\"pip\",\"install\",\"-q\",\"sacrebleu\",\"rouge-score\",\"evaluate\"])\n\ntry:\n    from sacrebleu import BLEU\n    from rouge_score import rouge_scorer\n    import evaluate as _hf_eval\n    METRICS_OK = True\nexcept Exception:\n    _install_metrics()\n    try:\n        from sacrebleu import BLEU\n        from rouge_score import rouge_scorer\n        import evaluate as _hf_eval\n        METRICS_OK = True\n    except Exception:\n        METRICS_OK = False\n        BLEU = None; rouge_scorer = None; _hf_eval = None\n\nclass MetricsCalculator:\n    def __init__(self, vocab):\n        self.vocab = vocab\n        self.idx2word = {v:k for k,v in vocab.items()}\n        if METRICS_OK:\n            self.bleu = BLEU()\n            self.rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n            self.chrf = _hf_eval.load(\"chrf\")\n        else:\n            self.bleu = self.rouge = self.chrf = None\n\n    def ids_to_text(self, ids):\n        out = []\n        for x in ids:\n            if x == self.vocab[\"<eos>\"] or x == self.vocab[\"<pad>\"]: break\n            if x == self.vocab[\"<bos>\"]: continue\n            out.append(self.idx2word.get(int(x), \"<unk>\"))\n        return \" \".join(out)\n\n    def bleu_score(self, preds, refs):\n        if not self.bleu:\n            p = [\" \".join(set(a.split())) for a in preds]\n            r = [\" \".join(set(b.split())) for b in refs]\n            match = sum(len(set(a.split()) & set(b.split())) for a,b in zip(p,r))\n            denom = sum(max(len(set(a.split())), len(set(b.split()))) for a,b in zip(p,r)) or 1\n            return 100.0*match/denom\n        return self.bleu.corpus_score(preds, [refs]).score\n\n    def rougeL(self, preds, refs):\n        if not self.rouge:\n            inter = [len(set(a.split()) & set(b.split()))/(len(set(b.split())) or 1) for a,b in zip(preds,refs)]\n            return 100.0*float(np.mean(inter)) if inter else 0.0\n        scores = [self.rouge.score(r, p)[\"rougeL\"].fmeasure for p,r in zip(preds,refs)]\n        return 100.0*float(np.mean(scores)) if scores else 0.0\n\n    def chrf_score(self, preds, refs):\n        if not self.chrf:\n            vals = []\n            for p,r in zip(preds,refs):\n                A,B=set(p),set(r)\n                if not B: vals.append(0.0); continue\n                prec = len(A&B)/(len(A) or 1); rec = len(A&B)/len(B)\n                f1 = (2*prec*rec)/(prec+rec) if (prec+rec)>0 else 0.0\n                vals.append(100.0*f1)\n            return float(np.mean(vals)) if vals else 0.0\n        return float(self.chrf.compute(predictions=preds, references=refs)[\"score\"])\n\n# ---------------------- Train / Eval ------------------------\ndef calculate_perplexity(loss_val):\n    return float(torch.exp(torch.tensor(loss_val)).item())\n\ndef train_epoch(model, loader, opt, criterion, device, pad_idx):\n    model.train()\n    total = 0.0\n    for src, tgt in tqdm(loader, desc=\"Training\"):\n        src, tgt = src.to(device), tgt.to(device)\n\n        # Teacher forcing (shift)\n        inp = tgt[:, :-1]\n        out = tgt[:, 1:]\n\n        opt.zero_grad()\n        logits = model(src, inp)\n        loss = criterion(logits.reshape(-1, logits.size(-1)), out.reshape(-1))\n        if torch.isnan(loss): raise ValueError(\"NaN loss detected.\")\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n        opt.step()\n        total += loss.item()\n    return total/len(loader)\n\ndef evaluate(model, loader, criterion, device, vocab, metrics: MetricsCalculator):\n    model.eval()\n    total = 0.0\n    preds, refs = [], []\n    with torch.no_grad():\n        for src, tgt in tqdm(loader, desc=\"Evaluating\"):\n            src, tgt = src.to(device), tgt.to(device)\n            inp, out = tgt[:, :-1], tgt[:, 1:]\n            logits = model(src, inp)\n            loss = criterion(logits.reshape(-1, logits.size(-1)), out.reshape(-1))\n            total += loss.item()\n\n            # generate (greedy) for metrics\n            gen = greedy_decode(model, src, vocab, max_len=cfg.max_seq_len, device=device)  # [B,T]\n            for i in range(src.size(0)):\n                preds.append(metrics.ids_to_text(gen[i].cpu().numpy()))\n                refs.append(metrics.ids_to_text(out[i].cpu().numpy()))\n    avg_loss = total/len(loader)\n    ppl = calculate_perplexity(avg_loss)\n    bleu = metrics.bleu_score(preds, refs)\n    rougeL = metrics.rougeL(preds, refs)\n    chrf = metrics.chrf_score(preds, refs)\n    return avg_loss, ppl, bleu, rougeL, chrf\n\ndef save_ckpt(path, model, opt, epoch, extra):\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    torch.save({\n        \"epoch\": epoch,\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": opt.state_dict(),\n        **extra\n    }, path)\n\n# --------------------------- Main ---------------------------\ndef main():\n    print(f\"Device: {cfg.device}\")\n    os.makedirs(cfg.save_dir, exist_ok=True)\n\n    # ---- Load vocab\n    with open(os.path.join(cfg.data_dir, \"vocab.json\"), \"r\") as f:\n        vocab = json.load(f)\n    assert all(k in vocab for k in [\"<pad>\",\"<bos>\",\"<eos>\"]), \"vocab.json must contain <pad>, <bos>, <eos>\"\n    pad = vocab[\"<pad>\"]\n\n    # ---- Datasets / Loaders\n    train_ds = EmpatheticDataset(os.path.join(cfg.data_dir,\"train_ids.jsonl\"), vocab, cfg.max_seq_len)\n    val_ds   = EmpatheticDataset(os.path.join(cfg.data_dir,\"val_ids.jsonl\"),   vocab, cfg.max_seq_len)\n    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n                              collate_fn=train_ds.collate_fn, num_workers=0)\n    val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False,\n                              collate_fn=val_ds.collate_fn, num_workers=0)\n\n    # ---- Model / Optimizer / Loss\n    model = TransformerModel(vocab_size=len(vocab), d_model=cfg.d_model, n_heads=cfg.n_heads,\n                             num_layers=cfg.num_layers, d_ff=cfg.d_ff, dropout=cfg.dropout,\n                             pad_idx=pad).to(cfg.device)\n    opt = optim.Adam(model.parameters(), lr=cfg.lr, betas=cfg.betas)\n    criterion = nn.CrossEntropyLoss(ignore_index=pad)\n\n    metrics_calc = MetricsCalculator(vocab)\n\n    best_bleu = -1.0\n    best_epoch = -1\n    history = []\n\n    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    print(\"Start training...\")\n    for epoch in range(1, cfg.epochs+1):\n        print(f\"\\nEpoch {epoch}/{cfg.epochs}\")\n        tr_loss = train_epoch(model, train_loader, opt, criterion, cfg.device, pad)\n        vl_loss, ppl, bleu, rougeL, chrf = evaluate(model, val_loader, criterion, cfg.device, vocab, metrics_calc)\n\n        print(f\"Train Loss: {tr_loss:.4f}\")\n        print(f\"Val   Loss: {vl_loss:.4f} | PPL: {ppl:.2f}\")\n        print(f\"BLEU: {bleu:.2f} | ROUGE-L: {rougeL:.2f} | chrF: {chrf:.2f}\")\n\n        # save epoch checkpoint\n        save_ckpt(\n            os.path.join(cfg.save_dir, f\"checkpoint_epoch_{epoch}.pt\"),\n            model, opt, epoch,\n            extra={\n                \"metrics\": {\"val_loss\":vl_loss,\"ppl\":ppl,\"bleu\":bleu,\"rougeL\":rougeL,\"chrf\":chrf},\n                \"config\": vars(cfg),\n                \"vocab\": vocab\n            }\n        )\n\n        # save best (by BLEU)\n        if bleu > best_bleu:\n            best_bleu = bleu\n            best_epoch = epoch\n            save_ckpt(\n                os.path.join(cfg.save_dir, \"best_model.pt\"),\n                model, opt, epoch,\n                extra={\n                    \"metrics\": {\"val_loss\":vl_loss,\"ppl\":ppl,\"bleu\":bleu,\"rougeL\":rougeL,\"chrf\":chrf},\n                    \"config\": vars(cfg),\n                    \"vocab\": vocab\n                }\n            )\n            print(f\"🏆 New best model saved at checkpoints/best_model.pt (BLEU {bleu:.2f})\")\n        else:\n            print(f\"📈 Best BLEU so far: {best_bleu:.2f} (epoch {best_epoch})\")\n\n        history.append({\n            \"epoch\": epoch,\n            \"train_loss\": tr_loss,\n            \"val_loss\": vl_loss,\n            \"perplexity\": ppl,\n            \"bleu\": bleu,\n            \"rouge_l\": rougeL,\n            \"chrf\": chrf\n        })\n\n    with open(os.path.join(cfg.save_dir,\"training_history.json\"),\"w\") as f:\n        json.dump(history, f, indent=2)\n\n    print(\"\\nFiles in checkpoints/:\")\n    for fn in sorted(os.listdir(cfg.save_dir)):\n        print(\" -\", fn)\n\n    print(f\"\\n✅ Training complete. Best BLEU: {best_bleu:.2f} (epoch {best_epoch})\")\n    print(f\"Best model path: {os.path.join(cfg.save_dir, 'best_model.pt')}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:13:48.126591Z","iopub.execute_input":"2025-10-17T17:13:48.126816Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nParameters: 14,487,102\nStart training...\n\nEpoch 1/6\n","output_type":"stream"},{"name":"stderr","text":"Training:  42%|████▏     | 683/1615 [00:22<00:32, 28.84it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# **Task 5 - Evaluation**","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport json\nimport os\nfrom tqdm import tqdm\nimport numpy as np\nimport math\nimport torch.nn.functional as F\nimport pandas as pd\nfrom collections import defaultdict\nimport random\n\n# ===== METRICS SETUP (robust, auto-install) =====\nMETRICS_AVAILABLE = False\ndef ensure_metrics():\n    \"\"\"Auto-install and import metrics packages (sacrebleu + rouge-score)\"\"\"\n    global METRICS_AVAILABLE, BLEU, CHRF, rouge_scorer\n    try:\n        from sacrebleu import BLEU\n        from sacrebleu.metrics import CHRF\n        from rouge_score import rouge_scorer\n        METRICS_AVAILABLE = True\n        return\n    except Exception:\n        print(\"Installing metrics packages...\")\n        import sys, subprocess, importlib\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"sacrebleu\", \"rouge-score\"])\n        importlib.invalidate_caches()\n        from sacrebleu import BLEU\n        from sacrebleu.metrics import CHRF\n        from rouge_score import rouge_scorer\n        METRICS_AVAILABLE = True\n        print(\"✓ Metrics packages installed\")\n\nensure_metrics()\n\n# ===== EMBEDDED MODEL ARCHITECTURE =====\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1), :]\n\n# Helper to prevent all-False mask rows (causes NaN in softmax)\ndef ensure_nonempty_rows(mask):\n    \"\"\"Ensure each row has at least one True value to prevent NaN in softmax\"\"\"\n    # mask: [B, Lq, Lk] boolean\n    row_has_true = mask.any(dim=-1, keepdim=True)  # [B, Lq, 1]\n    # For rows with all False, set first column to True\n    mask = mask.clone()\n    mask[:, :, 0] = mask[:, :, 0] | (~row_has_true.squeeze(-1))\n    return mask\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, n_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % n_heads == 0\n        self.h = n_heads\n        self.dk = d_model // n_heads\n        self.Wq = nn.Linear(d_model, d_model)\n        self.Wk = nn.Linear(d_model, d_model)\n        self.Wv = nn.Linear(d_model, d_model)\n        self.Wo = nn.Linear(d_model, d_model)\n        self.drop = nn.Dropout(dropout)\n    \n    def forward(self, q, k, v, mask=None):\n        B, L, D = q.size()\n        Q = self.Wq(q).view(B, L, self.h, self.dk).transpose(1, 2)\n        K = self.Wk(k).view(B, -1, self.h, self.dk).transpose(1, 2)\n        V = self.Wv(v).view(B, -1, self.h, self.dk).transpose(1, 2)\n        \n        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.dk)\n        if mask is not None:\n            if mask.dtype == torch.bool:\n                mask = ensure_nonempty_rows(mask)  # Prevent all-False rows\n                scores = scores.masked_fill(~mask.unsqueeze(1), float(\"-inf\"))\n            else:\n                scores = scores + mask.unsqueeze(1)\n        \n        attn = F.softmax(scores, dim=-1)\n        attn = torch.nan_to_num(attn, nan=0.0)  # Safety: sanitize any NaN\n        out = attn @ V\n        out = out.transpose(1, 2).contiguous().view(B, L, D)\n        return self.Wo(self.drop(out))\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.ff = nn.Sequential(\n            nn.Linear(d_model, d_ff), nn.ReLU(), nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model), nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.ff(x)\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, n_heads, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.sa = MultiHeadAttention(d_model, n_heads, dropout)\n        self.ff = FeedForward(d_model, d_ff, dropout)\n        self.n1 = nn.LayerNorm(d_model)\n        self.n2 = nn.LayerNorm(d_model)\n    def forward(self, x, attn_mask=None):\n        x = self.n1(x + self.sa(x, x, x, attn_mask))\n        x = self.n2(x + self.ff(x))\n        return x\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, n_heads, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.msa = MultiHeadAttention(d_model, n_heads, dropout)\n        self.xattn = MultiHeadAttention(d_model, n_heads, dropout)\n        self.ff = FeedForward(d_model, d_ff, dropout)\n        self.n1 = nn.LayerNorm(d_model)\n        self.n2 = nn.LayerNorm(d_model)\n        self.n3 = nn.LayerNorm(d_model)\n    def forward(self, x, mem, self_mask=None, mem_mask=None):\n        x = self.n1(x + self.msa(x, x, x, self_mask))\n        x = self.n2(x + self.xattn(x, mem, mem, mem_mask))\n        x = self.n3(x + self.ff(x))\n        return x\n\nclass Encoder(nn.Module):\n    def __init__(self, d_model, n_heads, num_layers, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n    def forward(self, x, attn_mask=None):\n        for lyr in self.layers:\n            x = lyr(x, attn_mask)\n        return x\n\nclass Decoder(nn.Module):\n    def __init__(self, d_model, n_heads, num_layers, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n    def forward(self, x, mem, self_mask=None, mem_mask=None):\n        for lyr in self.layers:\n            x = lyr(x, mem, self_mask, mem_mask)\n        return x\n\ndef make_pad_mask(ids, pad_idx):\n    return (ids != pad_idx)\n\ndef make_attn_mask(valid_q, valid_k):\n    return valid_q.unsqueeze(2) & valid_k.unsqueeze(1)\n\ndef make_causal_mask(L, device):\n    return torch.tril(torch.ones(L, L, dtype=torch.bool, device=device))\n\nclass TransformerModel(nn.Module):\n    def __init__(self, vocab_size, d_model=256, n_heads=2, num_layers=2, d_ff=2048, dropout=0.1, pad_idx=0):\n        super().__init__()\n        self.pad_idx = pad_idx\n        self.d_model = d_model\n        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n        self.pos = PositionalEncoding(d_model)\n        self.encoder = Encoder(d_model, n_heads, num_layers, d_ff, dropout)\n        self.decoder = Decoder(d_model, n_heads, num_layers, d_ff, dropout)\n        self.out = nn.Linear(d_model, vocab_size)\n    \n    def forward(self, src, tgt):\n        device = src.device\n        B, Ls = src.size()\n        Lt = tgt.size(1)\n        \n        src_valid = make_pad_mask(src, self.pad_idx)\n        tgt_valid = make_pad_mask(tgt, self.pad_idx)\n        enc_self = make_attn_mask(src_valid, src_valid)\n        dec_caus = make_causal_mask(Lt, device)\n        dec_self = dec_caus.unsqueeze(0).expand(B, -1, -1) & make_attn_mask(tgt_valid, tgt_valid)\n        cross = make_attn_mask(tgt_valid, src_valid)\n        \n        src_e = self.pos(self.embed(src) * math.sqrt(self.d_model))\n        tgt_e = self.pos(self.embed(tgt) * math.sqrt(self.d_model))\n        \n        mem = self.encoder(src_e, enc_self)\n        dec = self.decoder(tgt_e, mem, dec_self, cross)\n        return self.out(dec)\n\n# ===== GREEDY DECODING =====\ndef greedy_decode(model, src, vocab, max_len=128, device=None):\n    \"\"\"Generate response using greedy decoding (with safe length limiting)\"\"\"\n    if device is None:\n        device = src.device\n    \n    model.eval()\n    batch_size = src.size(0)\n    bos_token = vocab['<bos>']\n    eos_token = vocab['<eos>']\n    \n    tgt = torch.full((batch_size, 1), bos_token, dtype=torch.long, device=device)\n    \n    with torch.no_grad():\n        for _ in range(max_len - 1):  # Cap at max_len-1 to account for BOS\n            output = model(src, tgt)\n            next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)\n            tgt = torch.cat([tgt, next_token], dim=1)\n            if (next_token.squeeze(-1) == eos_token).all():\n                break\n    \n    # Return sequences stripped of BOS\n    return tgt[:, 1:]  # Remove BOS token\n\n# ===== DATASET =====\nclass ChatbotDataset(Dataset):\n    def __init__(self, jsonl_file, vocab, max_len=128):\n        self.data = []\n        self.vocab = vocab\n        self.max_len = max_len\n        self.pad_token = vocab['<pad>']\n        \n        with open(jsonl_file, 'r') as f:\n            for line in f:\n                item = json.loads(line.strip())\n                src_ids = item['src_ids'][:max_len]\n                tgt_ids = item['tgt_ids'][:max_len]\n                self.data.append({'src': src_ids, 'tgt': tgt_ids})\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx]\n    \n    def collate_fn(self, batch):\n        src_seqs = [item['src'] for item in batch]\n        tgt_seqs = [item['tgt'] for item in batch]\n        \n        max_src_len = max(len(seq) for seq in src_seqs)\n        max_tgt_len = max(len(seq) for seq in tgt_seqs)\n        \n        src_batch = torch.full((len(batch), max_src_len), self.pad_token, dtype=torch.long)\n        tgt_batch = torch.full((len(batch), max_tgt_len), self.pad_token, dtype=torch.long)\n        \n        for i, (src, tgt) in enumerate(zip(src_seqs, tgt_seqs)):\n            src_batch[i, :len(src)] = torch.tensor(src)\n            tgt_batch[i, :len(tgt)] = torch.tensor(tgt)\n        \n        return src_batch, tgt_batch\n\n# ===== METRICS CALCULATOR =====\nclass MetricsCalculator:\n    def __init__(self, vocab):\n        self.vocab = vocab\n        self.idx2word = {v: k for k, v in vocab.items()}\n        \n        # Initialize metrics (guaranteed available after ensure_metrics())\n        self.bleu = BLEU()\n        self.rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n        self.chrf_metric = CHRF()\n    \n    def ids_to_text(self, ids):\n        \"\"\"Convert token IDs to text\"\"\"\n        tokens = []\n        for id in ids:\n            id = int(id)  # Cast to plain int (safer with numpy types)\n            if id == self.vocab['<eos>'] or id == self.vocab['<pad>']:\n                break\n            if id == self.vocab['<bos>']:\n                continue\n            tokens.append(self.idx2word.get(id, '<unk>'))\n        return ' '.join(tokens)\n    \n    def calculate_bleu(self, predictions, references):\n        \"\"\"Calculate BLEU score\"\"\"\n        pred_texts = [self.ids_to_text(pred) for pred in predictions]\n        ref_texts = [self.ids_to_text(ref) for ref in references]\n        \n        valid_pairs = [(p, r) for p, r in zip(pred_texts, ref_texts) if p.strip() and r.strip()]\n        if not valid_pairs:\n            return 0.0\n        \n        pred_texts, ref_texts = zip(*valid_pairs)\n        \n        try:\n            score = self.bleu.corpus_score(pred_texts, [ref_texts])\n            return score.score\n        except:\n            # Fallback: simple BLEU\n            return self._simple_bleu(pred_texts, ref_texts)\n    \n    def _simple_bleu(self, pred_texts, ref_texts):\n        matches = 0\n        total = 0\n        for pred, ref in zip(pred_texts, ref_texts):\n            pred_words = set(pred.split())\n            ref_words = set(ref.split())\n            if pred_words and ref_words:\n                matches += len(pred_words & ref_words)\n                total += max(len(pred_words), len(ref_words))\n        return (matches / total * 100) if total > 0 else 0.0\n    \n    def calculate_rouge_l(self, predictions, references):\n        \"\"\"Calculate ROUGE-L score\"\"\"\n        pred_texts = [self.ids_to_text(pred) for pred in predictions]\n        ref_texts = [self.ids_to_text(ref) for ref in references]\n        \n        scores = []\n        for pred, ref in zip(pred_texts, ref_texts):\n            if pred.strip() and ref.strip():\n                try:\n                    score = self.rouge_scorer.score(ref, pred)\n                    scores.append(score['rougeL'].fmeasure)\n                except:\n                    # Fallback\n                    pred_words = set(pred.lower().split())\n                    ref_words = set(ref.lower().split())\n                    if ref_words:\n                        scores.append(len(pred_words & ref_words) / len(ref_words))\n        \n        return np.mean(scores) if scores else 0.0\n    \n    def calculate_chrf(self, predictions, references):\n        \"\"\"Calculate chrF score using sacrebleu\"\"\"\n        pred_texts = [self.ids_to_text(p) for p in predictions]\n        ref_texts = [self.ids_to_text(r) for r in references]\n        \n        valid_pairs = [(p, r) for p, r in zip(pred_texts, ref_texts) if p.strip() and r.strip()]\n        if not valid_pairs:\n            return 0.0\n        \n        pred_texts, ref_texts = zip(*valid_pairs)\n        \n        try:\n            return self.chrf_metric.corpus_score(pred_texts, [ref_texts]).score\n        except:\n            # Fallback: character-level F1\n            scores = []\n            for pred, ref in zip(pred_texts, ref_texts):\n                pred_chars = set(pred.lower())\n                ref_chars = set(ref.lower())\n                if ref_chars:\n                    precision = len(pred_chars & ref_chars) / len(pred_chars) if pred_chars else 0\n                    recall = len(pred_chars & ref_chars) / len(ref_chars)\n                    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n                    scores.append(f1 * 100)\n            return np.mean(scores) if scores else 0.0\n\n# ===== COMPREHENSIVE EVALUATION =====\ndef evaluate_model(model, test_loader, vocab, metrics_calc, device, criterion=None):\n    \"\"\"\n    Comprehensive model evaluation\n    \n    Returns:\n        results: Dictionary with all metrics\n        examples: List of sample outputs for qualitative analysis\n    \"\"\"\n    model.eval()\n    \n    all_predictions = []\n    all_references = []\n    all_examples = []\n    total_loss = 0\n    num_batches = 0\n    \n    print(\"Running evaluation...\")\n    \n    with torch.no_grad():\n        for src, tgt in tqdm(test_loader, desc=\"Evaluating\"):\n            src, tgt = src.to(device), tgt.to(device)\n            \n            # Calculate loss if criterion provided\n            if criterion:\n                tgt_input = tgt[:, :-1]\n                tgt_output = tgt[:, 1:]\n                output = model(src, tgt_input)\n                loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n                total_loss += loss.item()\n                num_batches += 1\n            \n            # Generate predictions\n            generated = greedy_decode(model, src, vocab, max_len=128, device=device)\n            \n            # Process per-sample\n            for i in range(src.size(0)):\n                # Extract sequences\n                src_seq = src[i].cpu().numpy()\n                \n                # Trim generated sequence at first <eos> (extra safety for mixed-length batches)\n                pred_seq_t = generated[i].cpu()\n                eos_pos = (pred_seq_t == vocab['<eos>']).nonzero(as_tuple=False)\n                if len(eos_pos) > 0:\n                    pred_seq_t = pred_seq_t[:eos_pos[0].item()+1]\n                pred_seq = pred_seq_t.numpy()\n                \n                # Reference (drop BOS, cut at EOS/PAD)\n                ref = tgt[i, 1:].cpu()\n                eos_pos = (ref == vocab['<eos>']).nonzero(as_tuple=False)\n                if len(eos_pos) > 0:\n                    ref = ref[:eos_pos[0].item()]\n                ref = ref[ref != vocab['<pad>']].numpy()\n                \n                all_predictions.append(pred_seq)\n                all_references.append(ref)\n                \n                # Store examples for qualitative analysis\n                if len(all_examples) < 50:  # Keep 50 examples\n                    all_examples.append({\n                        'input': metrics_calc.ids_to_text(src_seq),\n                        'reference': metrics_calc.ids_to_text(ref),\n                        'generated': metrics_calc.ids_to_text(pred_seq)\n                    })\n    \n    # Calculate metrics\n    print(\"\\nCalculating metrics...\")\n    \n    results = {}\n    \n    # Perplexity (if loss available)\n    if criterion and num_batches > 0:\n        avg_loss = total_loss / num_batches\n        results['loss'] = avg_loss\n        results['perplexity'] = torch.exp(torch.tensor(avg_loss)).item()\n    \n    # BLEU\n    results['bleu'] = metrics_calc.calculate_bleu(all_predictions, all_references)\n    \n    # ROUGE-L\n    results['rouge_l'] = metrics_calc.calculate_rouge_l(all_predictions, all_references)\n    \n    # chrF\n    results['chrf'] = metrics_calc.calculate_chrf(all_predictions, all_references)\n    \n    return results, all_examples\n\n# ===== HUMAN EVALUATION FRAMEWORK =====\ndef create_human_evaluation_template(examples, output_file=\"human_eval_template.csv\"):\n    \"\"\"\n    Create a CSV template for human evaluation\n    \n    Criteria:\n    - Fluency: How natural/grammatical is the response? (1-5)\n    - Relevance: How relevant is the response to the input? (1-5)\n    - Adequacy: How well does it capture the meaning? (1-5)\n    \"\"\"\n    \n    # Select random subset for human evaluation\n    eval_samples = random.sample(examples, min(100, len(examples)))\n    \n    data = []\n    for idx, ex in enumerate(eval_samples, 1):\n        data.append({\n            'ID': idx,\n            'Input': ex['input'],\n            'Reference': ex['reference'],\n            'Generated': ex['generated'],\n            'Fluency (1-5)': '',\n            'Relevance (1-5)': '',\n            'Adequacy (1-5)': '',\n            'Comments': ''\n        })\n    \n    df = pd.DataFrame(data)\n    df.to_csv(output_file, index=False)\n    \n    print(f\"✓ Human evaluation template saved: {output_file}\")\n    print(f\"  {len(data)} samples ready for annotation\")\n    print(\"\\nEvaluation Guidelines:\")\n    print(\"  Fluency (1-5):\")\n    print(\"    5 = Perfect, native-like\")\n    print(\"    4 = Good, minor errors\")\n    print(\"    3 = Acceptable, some errors\")\n    print(\"    2 = Poor, many errors\")\n    print(\"    1 = Incomprehensible\")\n    print(\"\\n  Relevance (1-5):\")\n    print(\"    5 = Perfectly relevant\")\n    print(\"    4 = Mostly relevant\")\n    print(\"    3 = Somewhat relevant\")\n    print(\"    2 = Barely relevant\")\n    print(\"    1 = Completely irrelevant\")\n    print(\"\\n  Adequacy (1-5):\")\n    print(\"    5 = Fully captures meaning\")\n    print(\"    4 = Captures most meaning\")\n    print(\"    3 = Partial meaning\")\n    print(\"    2 = Little meaning\")\n    print(\"    1 = No meaning captured\")\n    \n    return df\n\ndef analyze_human_evaluation(eval_file=\"human_eval_completed.csv\"):\n    \"\"\"Analyze completed human evaluation\"\"\"\n    df = pd.read_csv(eval_file)\n    \n    # Calculate averages\n    fluency_avg = df['Fluency (1-5)'].mean()\n    relevance_avg = df['Relevance (1-5)'].mean()\n    adequacy_avg = df['Adequacy (1-5)'].mean()\n    \n    print(\"=\"*70)\n    print(\"HUMAN EVALUATION RESULTS\")\n    print(\"=\"*70)\n    print(f\"Samples evaluated: {len(df)}\")\n    print(f\"\\nAverage Scores:\")\n    print(f\"  Fluency:   {fluency_avg:.2f} / 5.0\")\n    print(f\"  Relevance: {relevance_avg:.2f} / 5.0\")\n    print(f\"  Adequacy:  {adequacy_avg:.2f} / 5.0\")\n    print(f\"  Overall:   {(fluency_avg + relevance_avg + adequacy_avg) / 3:.2f} / 5.0\")\n    \n    # Score distribution\n    print(f\"\\nScore Distribution:\")\n    for metric in ['Fluency (1-5)', 'Relevance (1-5)', 'Adequacy (1-5)']:\n        print(f\"\\n{metric}:\")\n        counts = df[metric].value_counts().sort_index()\n        for score, count in counts.items():\n            percentage = (count / len(df)) * 100\n            print(f\"  {int(score)}: {'█' * int(percentage/2)} {count} ({percentage:.1f}%)\")\n    \n    return {\n        'fluency': fluency_avg,\n        'relevance': relevance_avg,\n        'adequacy': adequacy_avg,\n        'overall': (fluency_avg + relevance_avg + adequacy_avg) / 3\n    }\n\n# ===== QUALITATIVE ANALYSIS =====\ndef display_qualitative_examples(examples, num_examples=10, save_file=None):\n    \"\"\"Display qualitative comparison of model outputs\"\"\"\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"QUALITATIVE EXAMPLES: Model Output vs Ground Truth\")\n    print(\"=\"*70)\n    \n    # Select diverse examples\n    selected = random.sample(examples, min(num_examples, len(examples)))\n    \n    results = []\n    for idx, ex in enumerate(selected, 1):\n        print(f\"\\n{'─'*70}\")\n        print(f\"Example {idx}:\")\n        print(f\"{'─'*70}\")\n        print(f\"📥 Input:     {ex['input']}\")\n        print(f\"✅ Reference: {ex['reference']}\")\n        print(f\"🤖 Generated: {ex['generated']}\")\n        \n        # Simple quality indicators\n        ref_words = set(ex['reference'].lower().split())\n        gen_words = set(ex['generated'].lower().split())\n        overlap = len(ref_words & gen_words) / len(ref_words) if ref_words else 0\n        \n        print(f\"📊 Word Overlap: {overlap*100:.1f}%\")\n        \n        results.append({\n            'Example': idx,\n            'Input': ex['input'],\n            'Reference': ex['reference'],\n            'Generated': ex['generated'],\n            'Word Overlap': f\"{overlap*100:.1f}%\"\n        })\n    \n    if save_file:\n        df = pd.DataFrame(results)\n        df.to_csv(save_file, index=False)\n        print(f\"\\n✓ Examples saved to: {save_file}\")\n    \n    return results\n\n# ===== MAIN EVALUATION FUNCTION =====\ndef run_full_evaluation(checkpoint_path, data_dir, output_dir=\"/kaggle/working/evaluation\"):\n    \"\"\"\n    Run complete evaluation pipeline\n    \n    Args:\n        checkpoint_path: Path to trained model checkpoint\n        data_dir: Directory with vocab.json and test_ids.jsonl\n        output_dir: Where to save evaluation results\n    \n    Returns:\n        metrics: Dictionary of all metrics\n        examples: Qualitative examples\n    \"\"\"\n    print(\"=\"*70)\n    print(\"MODEL EVALUATION\")\n    print(\"=\"*70)\n    \n    # Setup\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Device: {device}\")\n    \n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Load vocabulary\n    vocab_path = f\"{data_dir}/vocab.json\"\n    print(f\"Loading vocab: {vocab_path}\")\n    with open(vocab_path, \"r\") as f:\n        vocab = json.load(f)\n    print(f\"✓ Vocab size: {len(vocab)}\")\n    \n    # Load model checkpoint first\n    print(f\"Loading model: {checkpoint_path}\")\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    \n    # Prefer vocab from checkpoint if present (ensures ID consistency)\n    if \"vocab\" in checkpoint and isinstance(checkpoint[\"vocab\"], dict):\n        print(\"✓ Using vocab from checkpoint (ensures ID consistency)\")\n        vocab = checkpoint[\"vocab\"]\n    \n    # Load test data\n    print(\"Loading test data...\")\n    test_dataset = ChatbotDataset(f\"{data_dir}/test_ids.jsonl\", vocab)\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=32,\n        shuffle=False,\n        collate_fn=test_dataset.collate_fn,\n        num_workers=0\n    )\n    print(f\"✓ Test samples: {len(test_dataset)}\")\n    \n    # Extract model config from checkpoint (if available) or use defaults\n    config = checkpoint.get('config', {})\n    d_model = config.get('d_model', 256)\n    n_heads = config.get('n_heads', 2)\n    num_layers = config.get('num_layers', 2)\n    d_ff = config.get('d_ff', 2048)\n    dropout = config.get('dropout', 0.1)\n    \n    print(f\"✓ Model config: d_model={d_model}, n_heads={n_heads}, num_layers={num_layers}\")\n    \n    model = TransformerModel(\n        vocab_size=len(vocab),\n        d_model=d_model,\n        n_heads=n_heads,\n        num_layers=num_layers,\n        d_ff=d_ff,\n        dropout=dropout,\n        pad_idx=vocab['<pad>']\n    ).to(device)\n    \n    model.load_state_dict(checkpoint['model_state_dict'])\n    print(f\"✓ Model loaded (epoch {checkpoint.get('epoch', 'N/A')})\")\n    \n    # Initialize metrics calculator\n    metrics_calc = MetricsCalculator(vocab)\n    criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n    \n    # Run evaluation\n    print(\"\\n\" + \"=\"*70)\n    results, examples = evaluate_model(\n        model, test_loader, vocab, metrics_calc, device, criterion\n    )\n    \n    # Display results\n    print(\"\\n\" + \"=\"*70)\n    print(\"RESULTS\")\n    print(\"=\"*70)\n    if 'perplexity' in results:\n        print(f\"Perplexity: {results['perplexity']:.2f}\")\n    print(f\"BLEU:       {results['bleu']:.2f}\")\n    print(f\"ROUGE-L:    {results['rouge_l']:.4f}\")\n    print(f\"chrF:       {results['chrf']:.2f}\")\n    \n    # Save metrics\n    metrics_file = f\"{output_dir}/automatic_metrics.json\"\n    with open(metrics_file, 'w') as f:\n        json.dump(results, f, indent=2)\n    print(f\"\\n✓ Saved: {metrics_file}\")\n    \n    # Generate qualitative examples\n    print(\"\\nGenerating examples...\")\n    examples_file = f\"{output_dir}/qualitative_examples.csv\"\n    display_qualitative_examples(examples, num_examples=20, save_file=examples_file)\n    \n    # Create human evaluation template\n    print(\"\\nCreating human eval template...\")\n    human_eval_file = f\"{output_dir}/human_evaluation_template.csv\"\n    create_human_evaluation_template(examples, human_eval_file)\n    \n    # Summary\n    print(\"\\n\" + \"=\"*70)\n    print(\"COMPLETE!\")\n    print(\"=\"*70)\n    print(f\"Output directory: {output_dir}\")\n    print(f\"  1. {os.path.basename(metrics_file)}\")\n    print(f\"  2. {os.path.basename(examples_file)}\")\n    print(f\"  3. {os.path.basename(human_eval_file)}\")\n    print(\"=\"*70)\n    \n    return results, examples\n\n# ===== AUTO-DETECTION HELPERS =====\ndef find_checkpoint_file(prefer_best=True):\n    \"\"\"\n    Search for a .pt model in both /kaggle/input and /kaggle/working.\n    \n    Args:\n        prefer_best: If True, prioritize best_model.pt over epoch checkpoints\n    \n    Returns:\n        Path to the best available checkpoint\n    \"\"\"\n    roots = [\"/kaggle/input\", \"/kaggle/working\"]\n    \n    # First priority: best_model.pt (BLEU-selected)\n    if prefer_best:\n        for base in roots:\n            if not os.path.exists(base):\n                continue\n            for root, _, files in os.walk(base):\n                if \"best_model.pt\" in files:\n                    return os.path.join(root, \"best_model.pt\")\n    \n    # Second priority: any checkpoint file\n    for base in roots:\n        if not os.path.exists(base):\n            continue\n        for root, _, files in os.walk(base):\n            for f in files:\n                if f.endswith(\".pt\") and (\"checkpoint\" in f.lower() or \"model\" in f.lower()):\n                    return os.path.join(root, f)\n    \n    return None\n\ndef find_data_directory():\n    \"\"\"Look for a folder that has vocab.json and test_ids.jsonl in input/working.\"\"\"\n    candidates = [\n        \"/kaggle/input\", \"/kaggle/working\",\n        \"/kaggle/working/checkpoints\", \"/kaggle/input/checkpoints\"\n    ]\n    for base in candidates:\n        if not os.path.exists(base):\n            continue\n        for root, dirs, files in os.walk(base):\n            if \"vocab.json\" in files and \"test_ids.jsonl\" in files:\n                return root\n    return None\n\ndef auto_detect_paths():\n    \"\"\"\n    Automatically detect checkpoint and data paths in Kaggle.\n    Prioritizes best_model.pt (BLEU-selected) over epoch checkpoints.\n    \"\"\"\n    checkpoint_path = find_checkpoint_file(prefer_best=True)\n    data_dir = find_data_directory()\n    \n    if checkpoint_path:\n        print(f\"✓ Found checkpoint: {checkpoint_path}\")\n    else:\n        print(\"❌ No checkpoint file found (.pt)\")\n    \n    if data_dir:\n        print(f\"✓ Found data directory: {data_dir}\")\n    else:\n        print(\"❌ No data directory found (needs vocab.json + test_ids.jsonl)\")\n    \n    return checkpoint_path, data_dir\n\n# ============================================================\n# MAIN EXECUTION\n# ============================================================\n\nif __name__ == \"__main__\":\n    print(\"=\"*70)\n    print(\"EMPATHETIC CHATBOT - MODEL EVALUATION\")\n    print(\"=\"*70)\n    print(\"✓ Metrics packages available\")\n    \n    # Check if in Kaggle\n    if os.path.exists(\"/kaggle/input\"):\n        print(\"✓ Running in Kaggle environment\\n\")\n        \n        # AUTO-DETECT PATHS (prioritizes best_model.pt)\n        print(\"Auto-detecting paths...\")\n        checkpoint_path, data_dir = auto_detect_paths()\n        print()\n        \n        # Verify both found\n        if checkpoint_path and data_dir:\n            output_dir = \"/kaggle/working/evaluation\"\n            \n            # RUN EVALUATION\n            try:\n                metrics, examples = run_full_evaluation(\n                    checkpoint_path=checkpoint_path,\n                    data_dir=data_dir,\n                    output_dir=output_dir\n                )\n                print(\"\\n✓ Evaluation complete!\")\n                \n            except Exception as e:\n                print(f\"\\n✗ Error: {e}\")\n                import traceback\n                traceback.print_exc()\n        else:\n            print(\"✗ Missing files:\")\n            if not checkpoint_path:\n                print(\"  - Model checkpoint (.pt file)\")\n            if not data_dir:\n                print(\"  - Data (vocab.json + test_ids.jsonl)\")\n            print(\"\\nAdd datasets in Kaggle and re-run.\")\n    else:\n        print(\"✓ Running locally\\n\")\n        print(\"Usage:\")\n        print(\"  metrics, examples = run_full_evaluation(\")\n        print(\"      checkpoint_path='checkpoints/best_model.pt',\")\n        print(\"      data_dir='.',\")\n        print(\"      output_dir='./evaluation'\")\n        print(\"  )\")\n        print(\"\\nOr see examples in the docstring at the bottom of this file.\")\n\n# ============================================================\n# QUICK USAGE EXAMPLES\n# ============================================================\n\"\"\"\nEXAMPLE 1: Run full evaluation in Kaggle\n=========================================\nmetrics, examples = run_full_evaluation(\n    checkpoint_path=\"/kaggle/input/my-model/best_model.pt\",\n    data_dir=\"/kaggle/input/my-data\",\n    output_dir=\"/kaggle/working/evaluation\"\n)\n\nEXAMPLE 2: Just metrics (no files)\n===================================\ndevice = torch.device('cuda')\nvocab = json.load(open(\"vocab.json\"))\nmodel = TransformerModel(len(vocab), pad_idx=vocab['<pad>']).to(device)\ncheckpoint = torch.load(\"best_model.pt\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\ntest_dataset = ChatbotDataset(\"test_ids.jsonl\", vocab)\ntest_loader = DataLoader(test_dataset, batch_size=32, collate_fn=test_dataset.collate_fn)\nmetrics_calc = MetricsCalculator(vocab)\ncriterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n\nresults, examples = evaluate_model(model, test_loader, vocab, metrics_calc, device, criterion)\nprint(f\"BLEU: {results['bleu']:.2f}\")\nprint(f\"ROUGE-L: {results['rouge_l']:.4f}\")\nprint(f\"chrF: {results['chrf']:.2f}\")\n\nEXAMPLE 3: Analyze human evaluation results\n============================================\nhuman_scores = analyze_human_evaluation(\"/kaggle/input/human-eval/completed.csv\")\nprint(f\"Overall human score: {human_scores['overall']:.2f}/5.0\")\n\"\"\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Inference Script**","metadata":{}},{"cell_type":"code","source":"# # ============================================================\n# # Chatbot Inference Script — loads checkpoints/best_model.pt\n# # ============================================================\n\n# import os, json, math, sys\n# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n\n# # ------------------- Model (same as training) -------------------\n# class PositionalEncoding(nn.Module):\n#     def __init__(self, d_model, max_len=5000):\n#         super().__init__()\n#         pe = torch.zeros(max_len, d_model)\n#         pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n#         div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))\n#         pe[:, 0::2] = torch.sin(pos * div)\n#         pe[:, 1::2] = torch.cos(pos * div)\n#         self.register_buffer(\"pe\", pe.unsqueeze(0))\n#     def forward(self, x):\n#         return x + self.pe[:, :x.size(1), :]\n\n# def make_pad_mask(ids, pad_idx):  # True=valid token\n#     return (ids != pad_idx)\n\n# def make_attn_mask(valid_q, valid_k):  # [B,Lq,Lk]\n#     return valid_q.unsqueeze(2) & valid_k.unsqueeze(1)\n\n# def make_causal_mask(L, device):\n#     return torch.tril(torch.ones(L, L, dtype=torch.bool, device=device))\n\n# def _ensure_nonempty_rows(mask):\n#     has_true = mask.any(dim=-1, keepdim=True)\n#     mask = mask.clone()\n#     mask[:, :, 0] = mask[:, :, 0] | (~has_true.squeeze(-1))\n#     return mask\n\n# class MultiHeadAttention(nn.Module):\n#     def __init__(self, d_model, n_heads, dropout=0.1):\n#         super().__init__()\n#         assert d_model % n_heads == 0\n#         self.h = n_heads\n#         self.dk = d_model // n_heads\n#         self.Wq = nn.Linear(d_model, d_model)\n#         self.Wk = nn.Linear(d_model, d_model)\n#         self.Wv = nn.Linear(d_model, d_model)\n#         self.Wo = nn.Linear(d_model, d_model)\n#         self.drop = nn.Dropout(dropout)\n#     def _split(self, x):\n#         B,L,D = x.size()\n#         return x.view(B, L, self.h, self.dk).transpose(1,2)\n#     def _merge(self, x):\n#         B,h,L,dk = x.size()\n#         return x.transpose(1,2).contiguous().view(B,L,h*dk)\n#     def forward(self, q, k, v, mask=None):\n#         Q, K, V = self._split(self.Wq(q)), self._split(self.Wk(k)), self._split(self.Wv(v))\n#         scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.dk)\n#         if mask is not None:\n#             if mask.dtype == torch.bool:\n#                 mask = _ensure_nonempty_rows(mask)\n#                 scores = scores.masked_fill(~mask.unsqueeze(1), float(\"-inf\"))\n#             else:\n#                 scores = scores + mask.unsqueeze(1)\n#         attn = F.softmax(scores, dim=-1)\n#         attn = torch.nan_to_num(attn, nan=0.0)\n#         out = attn @ V\n#         out = self._merge(out)\n#         return self.Wo(self.drop(out))\n\n# class FeedForward(nn.Module):\n#     def __init__(self, d_model, d_ff=2048, dropout=0.1):\n#         super().__init__()\n#         self.ff = nn.Sequential(\n#             nn.Linear(d_model, d_ff), nn.ReLU(), nn.Dropout(dropout),\n#             nn.Linear(d_ff, d_model), nn.Dropout(dropout)\n#         )\n#     def forward(self, x): return self.ff(x)\n\n# class EncoderLayer(nn.Module):\n#     def __init__(self, d_model, n_heads, d_ff=2048, dropout=0.1):\n#         super().__init__()\n#         self.sa = MultiHeadAttention(d_model, n_heads, dropout)\n#         self.ff = FeedForward(d_model, d_ff, dropout)\n#         self.n1, self.n2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n#     def forward(self, x, attn_mask=None):\n#         x = self.n1(x + self.sa(x, x, x, attn_mask))\n#         x = self.n2(x + self.ff(x))\n#         return x\n\n# class DecoderLayer(nn.Module):\n#     def __init__(self, d_model, n_heads, d_ff=2048, dropout=0.1):\n#         super().__init__()\n#         self.msa = MultiHeadAttention(d_model, n_heads, dropout)\n#         self.xattn = MultiHeadAttention(d_model, n_heads, dropout)\n#         self.ff = FeedForward(d_model, d_ff, dropout)\n#         self.n1, self.n2, self.n3 = nn.LayerNorm(d_model), nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n#     def forward(self, x, mem, self_mask=None, mem_mask=None):\n#         x = self.n1(x + self.msa(x, x, x, self_mask))\n#         x = self.n2(x + self.xattn(x, mem, mem, mem_mask))\n#         x = self.n3(x + self.ff(x))\n#         return x\n\n# class Encoder(nn.Module):\n#     def __init__(self, d_model, n_heads, num_layers, d_ff=2048, dropout=0.1):\n#         super().__init__()\n#         self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n#     def forward(self, x, attn_mask=None):\n#         for lyr in self.layers: x = lyr(x, attn_mask)\n#         return x\n\n# class Decoder(nn.Module):\n#     def __init__(self, d_model, n_heads, num_layers, d_ff=2048, dropout=0.1):\n#         super().__init__()\n#         self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n#     def forward(self, x, mem, self_mask=None, mem_mask=None):\n#         for lyr in self.layers: x = lyr(x, mem, self_mask, mem_mask)\n#         return x\n\n# class TransformerModel(nn.Module):\n#     def __init__(self, vocab_size, d_model=256, n_heads=2, num_layers=2, d_ff=2048, dropout=0.1, pad_idx=0):\n#         super().__init__()\n#         self.pad_idx = pad_idx\n#         self.d_model = d_model\n#         self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n#         self.pos = PositionalEncoding(d_model)\n#         self.encoder = Encoder(d_model, n_heads, num_layers, d_ff, dropout)\n#         self.decoder = Decoder(d_model, n_heads, num_layers, d_ff, dropout)\n#         self.out = nn.Linear(d_model, vocab_size)\n#     def forward(self, src, tgt):\n#         device = src.device\n#         B, Ls = src.size()\n#         Lt = tgt.size(1)\n#         src_valid = make_pad_mask(src, self.pad_idx)\n#         tgt_valid = make_pad_mask(tgt, self.pad_idx)\n#         enc_self  = make_attn_mask(src_valid, src_valid)\n#         dec_caus  = make_causal_mask(Lt, device)\n#         dec_self  = dec_caus.unsqueeze(0).expand(B,-1,-1) & make_attn_mask(tgt_valid, tgt_valid)\n#         cross     = make_attn_mask(tgt_valid, src_valid)\n#         src_e = self.pos(self.embed(src) * math.sqrt(self.d_model))\n#         tgt_e = self.pos(self.embed(tgt) * math.sqrt(self.d_model))\n#         mem = self.encoder(src_e, enc_self)\n#         dec = self.decoder(tgt_e, mem, dec_self, cross)\n#         return self.out(dec)\n\n# # --------------------- Token utils ---------------------\n# def ids_to_text(ids, vocab):\n#     idx2word = {v:k for k,v in vocab.items()}\n#     out = []\n#     for x in ids:\n#         if x == vocab.get(\"<eos>\"): break\n#         if x == vocab.get(\"<pad>\"): break\n#         if x == vocab.get(\"<bos>\"): continue\n#         out.append(idx2word.get(int(x), \"<unk>\"))\n#     return \" \".join(out)\n\n# def text_to_ids(text, vocab):\n#     # simple whitespace tokenization (must match how vocab was built)\n#     # unknown tokens -> <unk>\n#     w2i = vocab\n#     unk = w2i.get(\"<unk>\", 0)\n#     tokens = text.strip().lower().split()\n#     return [w2i.get(t, unk) for t in tokens]\n\n# def build_source(emotion, situation, customer):\n#     # Matches project input template\n#     # Emotion optional; if empty, omit\n#     parts = []\n#     if emotion:\n#         parts.append(f\"emotion: {emotion}\")\n#     parts.append(f\"situation: {situation}\".strip())\n#     parts.append(f\"customer: {customer}\".strip())\n#     parts.append(\"agent:\")\n#     return \" | \".join(parts).lower()\n\n# # --------------------- Decoding ---------------------\n# @torch.no_grad()\n# def greedy_decode(model, src, vocab, max_len=128, device=None):\n#     if device is None: device = src.device\n#     model.eval()\n#     bos = vocab[\"<bos>\"]; eos = vocab[\"<eos>\"]\n#     B = src.size(0)\n#     tgt = torch.full((B,1), bos, dtype=torch.long, device=device)\n#     for _ in range(max_len-1):\n#         logits = model(src, tgt)\n#         next_tok = logits[:, -1, :].argmax(-1, keepdim=True)\n#         tgt = torch.cat([tgt, next_tok], dim=1)\n#         if (next_tok.squeeze(-1) == eos).all(): break\n#     return tgt[:,1:]  # strip BOS\n\n# @torch.no_grad()\n# def beam_search_decode(model, src, vocab, max_len=128, beam_size=4, alpha=0.7, device=None):\n#     \"\"\"\n#     Simple length-penalized beam search.\n#     score = sum(logprobs) / (len^alpha)\n#     \"\"\"\n#     if device is None: device = src.device\n#     model.eval()\n#     bos = vocab[\"<bos>\"]; eos = vocab[\"<eos>\"]\n\n#     beams = [(torch.tensor([bos], device=device, dtype=torch.long), 0.0, False)]  # (seq, score, ended)\n#     for _ in range(max_len-1):\n#         # gather candidates\n#         all_cands = []\n#         seqs = [b[0] for b in beams]\n#         # expand each beam independently\n#         for (seq, score, ended) in beams:\n#             if ended:\n#                 all_cands.append((seq, score, True))\n#                 continue\n#             tgt = seq.unsqueeze(0)                                 # [1, t]\n#             logits = model(src, tgt)                               # [1, t, V]\n#             logp = F.log_softmax(logits[:, -1, :], dim=-1)[0]      # [V]\n#             topk_logp, topk_ids = torch.topk(logp, beam_size)\n#             for lp, wid in zip(topk_logp.tolist(), topk_ids.tolist()):\n#                 new_seq = torch.cat([seq, torch.tensor([wid], device=device)])\n#                 ended2 = (wid == eos)\n#                 # length penalty\n#                 L = new_seq.size(0)\n#                 new_score = (score * ((L-1)**alpha) + lp) / (L**alpha)\n#                 all_cands.append((new_seq, new_score, ended2))\n#         # select best beams\n#         all_cands.sort(key=lambda x: x[1], reverse=True)\n#         beams = all_cands[:beam_size]\n#         # early stop if all ended\n#         if all(b[2] for b in beams):\n#             break\n\n#     best_seq = max(beams, key=lambda x: x[1])[0]\n#     # remove BOS\n#     if best_seq[0].item() == bos:\n#         best_seq = best_seq[1:]\n#     return best_seq.unsqueeze(0)  # [1, T]\n\n# # --------------------- Loader ---------------------\n# def load_vocab(vocab_path, ckpt_dict):\n#     if os.path.exists(vocab_path):\n#         with open(vocab_path, \"r\") as f:\n#             return json.load(f)\n#     # fallback: from checkpoint extras\n#     if isinstance(ckpt_dict.get(\"vocab\"), dict):\n#         return ckpt_dict[\"vocab\"]\n#     raise FileNotFoundError(\"vocab.json not found and 'vocab' not present in checkpoint.\")\n\n# def build_model_from_ckpt(ckpt, vocab, device):\n#     cfg = ckpt.get(\"config\", {}) or {}\n#     d_model   = int(cfg.get(\"d_model\", 256))\n#     n_heads   = int(cfg.get(\"n_heads\", 2))\n#     num_layers= int(cfg.get(\"num_layers\", 2))\n#     d_ff      = int(cfg.get(\"d_ff\", 2048))\n#     dropout   = float(cfg.get(\"dropout\", 0.1))\n#     pad_idx   = int(vocab.get(\"<pad>\", 0))\n\n#     model = TransformerModel(\n#         vocab_size=len(vocab),\n#         d_model=d_model,\n#         n_heads=n_heads,\n#         num_layers=num_layers,\n#         d_ff=d_ff,\n#         dropout=dropout,\n#         pad_idx=pad_idx\n#     ).to(device)\n#     model.load_state_dict(ckpt[\"model_state_dict\"], strict=True)\n#     model.eval()\n#     return model\n\n# # --------------------- Chat Loop ---------------------\n# def chat_loop(model, vocab, device, max_len=128, use_beam=False, beam_size=4):\n#     print(\"\\nEmpathetic Chatbot ready. Commands: /beam, /greedy, /quit\\n\")\n#     while True:\n#         try:\n#             emo = input(\"Emotion (optional, e.g., 'sad/afraid/hopeful'; Enter to skip): \").strip()\n#             sit = input(\"Situation: \").strip()\n#             usr = input(\"Customer: \").strip()\n#         except EOFError:\n#             print(\"\\nExiting.\")\n#             break\n\n#         # command shortcuts when user entered a command instead of text\n#         if emo.lower() in (\"/quit\",\"quit\",\"exit\"):\n#             break\n#         if emo.lower() == \"/beam\":\n#             use_beam = True\n#             print(\"Decoding set to BEAM.\")\n#             continue\n#         if emo.lower() == \"/greedy\":\n#             use_beam = False\n#             print(\"Decoding set to GREEDY.\")\n#             continue\n\n#         # Build input string\n#         src_text = build_source(emo, sit, usr)\n#         src_ids = text_to_ids(src_text, vocab)\n#         if len(src_ids) == 0:\n#             print(\"Input produced empty ids; please try again.\")\n#             continue\n\n#         src = torch.tensor(src_ids, dtype=torch.long, device=device).unsqueeze(0)  # [1, L]\n\n#         # Decode\n#         if use_beam:\n#             gen = beam_search_decode(model, src, vocab, max_len=max_len, beam_size=beam_size, device=device)  # [1,T]\n#         else:\n#             gen = greedy_decode(model, src, vocab, max_len=max_len, device=device)  # [1,T]\n\n#         reply = ids_to_text(gen[0].tolist(), vocab)\n#         print(f\"\\nAgent: {reply}\\n\")\n\n# # --------------------- Main ---------------------\n# def main():\n#     # Paths\n#     ckpt_path = os.environ.get(\"BEST_CKPT\", \"checkpoints/best_model.pt\")\n#     vocab_path = os.environ.get(\"VOCAB_JSON\", \"vocab.json\")\n\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#     print(f\"Using device: {device}\")\n\n#     if not os.path.exists(ckpt_path):\n#         print(f\"Checkpoint not found at: {ckpt_path}\")\n#         print(\"Set BEST_CKPT env var or place best_model.pt under checkpoints/.\")\n#         sys.exit(1)\n\n#     # Load checkpoint\n#     ckpt = torch.load(ckpt_path, map_location=device)\n\n#     # Load vocab (file first, fallback to ckpt['vocab'])\n#     vocab = load_vocab(vocab_path, ckpt)\n#     for tok in [\"<pad>\",\"<bos>\",\"<eos>\",\"<unk>\"]:\n#         if tok not in vocab:\n#             raise RuntimeError(f\"Missing special token in vocab: {tok}\")\n\n#     # Recreate model and load weights\n#     model = build_model_from_ckpt(ckpt, vocab, device)\n\n#     # Max gen length (from config if available)\n#     cfg = ckpt.get(\"config\", {}) or {}\n#     max_len = int(cfg.get(\"max_seq_len\", 128))\n\n#     # Start interactive chat\n#     chat_loop(model, vocab, device, max_len=max_len, use_beam=False, beam_size=4)\n\n# if __name__ == \"__main__\":\n#     main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **END**","metadata":{}}]}